{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import supervision as sv\n",
    "import open3d as o3d\n",
    "from cuml.cluster import DBSCAN\n",
    "import cupy as cp\n",
    "\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import box_convert\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"/scratch/kumaradi.gupta/checkpoints\"\n",
    "\n",
    "imgs_dir = \"/scratch/kumaradi.gupta/run_kinect_wheel_1/rgb\"\n",
    "depth_dir = \"/scratch/kumaradi.gupta/run_kinect_wheel_1/depth/\"\n",
    "pose_dir = \"/scratch/kumaradi.gupta/run_kinect_wheel_1/pose/\"\n",
    "\n",
    "img_dict_dir = \"/scratch/kumaradi.gupta/kinect_img_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pastel_color():\n",
    "    # generate (r, g, b) tuple of random numbers between 0.5 and 1, truncate to 2 decimal places\n",
    "    r = round(random.uniform(0.5, 1), 2)\n",
    "    g = round(random.uniform(0.5, 1), 2)\n",
    "    b = round(random.uniform(0.5, 1), 2)\n",
    "    return (r, g, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle file\n",
    "with open(img_dict_dir, 'rb') as file:\n",
    "    img_dict = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "img_dict = {img_name: {img_path: str,\n",
    "                        ram_tags: list_of_str,\n",
    "                        objs: {0: {bbox: [x1, y1, x2, y2],\n",
    "                                    phrase: str,\n",
    "                                    clip_embed: [1, 1024]},\n",
    "                                    dino_embed: [1, 1024]},\n",
    "                                    mask: [h, w],\n",
    "                                    prob: float,\n",
    "                                    aabb: arr}\n",
    "                                1: {...},\n",
    "                        }\n",
    "            img_name: {...},\n",
    "            }\n",
    "'''\n",
    "\n",
    "def get_depth(img_name):\n",
    "    # depth_path = os.path.join(depth_dir, img_name + '.npy')\n",
    "    # depth = np.load(depth_path)\n",
    "\n",
    "    depth_path = os.path.join(depth_dir, img_name + '.png')\n",
    "    depth = cv2.imread(depth_path, cv2.IMREAD_ANYDEPTH)\n",
    "    depth = depth.astype(np.float32) / 1000.0\n",
    "    return depth\n",
    "\n",
    "def get_pose(img_name):\n",
    "    pose_path = os.path.join(pose_dir, img_name + '.txt')\n",
    "\n",
    "    # check if the pose file exists, if it doesn't, return None\n",
    "    if not os.path.exists(pose_path):\n",
    "        return None\n",
    "    \n",
    "    with open(pose_path, 'r') as f:\n",
    "        pose = f.read().split()\n",
    "        pose = np.array(pose).astype(np.float32)\n",
    "    return pose\n",
    "\n",
    "def get_sim_cam_mat_with_fov(h, w, fov):\n",
    "    cam_mat = np.eye(3)\n",
    "    cam_mat[0, 0] = cam_mat[1, 1] = w / (2.0 * np.tan(np.deg2rad(fov / 2)))\n",
    "    cam_mat[0, 2] = w / 2.0\n",
    "    cam_mat[1, 2] = h / 2.0\n",
    "    return cam_mat\n",
    "\n",
    "def get_realsense_cam_mat():\n",
    "    K = np.array([[386.458, 0, 321.111],\n",
    "              [0, 386.458, 241.595],\n",
    "              [0, 0, 1]])\n",
    "    return K\n",
    "\n",
    "def get_kinect_cam_mat():\n",
    "    K = np.array([[9.7096624755859375e+02, 0., 1.0272059326171875e+03], \n",
    "                  [0., 9.7109600830078125e+02, 7.7529718017578125e+02], \n",
    "                  [0., 0., 1]])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_point_cloud(img_id, obj_data, cam_mat, color=(1, 0, 0), cam_height=0.9):\n",
    "    \"\"\"\n",
    "    Generates a point cloud from a depth image, camera intrinsics, mask, and pose.\n",
    "    Only points within the mask and with valid depth are added to the cloud.\n",
    "    Points are colored using the specified color.\n",
    "    \"\"\"\n",
    "    \n",
    "    depth = get_depth(img_id)\n",
    "    pose = get_pose(img_id)\n",
    "    mask = obj_data['mask']\n",
    "\n",
    "    if pose is None:\n",
    "        return o3d.geometry.PointCloud()\n",
    "\n",
    "    # Reproject the depth to 3D space\n",
    "    rows, cols = np.where(mask)\n",
    "\n",
    "    depth_values = depth[rows, cols]\n",
    "    valid_depth_indices = (depth_values > 0) & (depth_values <= 5)\n",
    "\n",
    "    rows = rows[valid_depth_indices]\n",
    "    cols = cols[valid_depth_indices]\n",
    "    depth_values = depth_values[valid_depth_indices]\n",
    "\n",
    "    points2d = np.vstack([cols, rows, np.ones_like(rows)])\n",
    "\n",
    "    cam_mat_inv = np.linalg.inv(cam_mat)\n",
    "    points3d_cam = cam_mat_inv @ points2d * depth_values\n",
    "\n",
    "    # Parse the pose\n",
    "    pos = np.array(pose[:3], dtype=float).reshape((3, 1))\n",
    "    quat = pose[3:]\n",
    "    rot = R.from_quat(quat).as_matrix()\n",
    "\n",
    "    # # Apply rotation correction, to match the orientation z: backward, y: upward, and x: right\n",
    "    # rot_ro_cam = np.eye(3)\n",
    "    # rot_ro_cam[1, 1] = -1\n",
    "    # rot_ro_cam[2, 2] = -1\n",
    "    # rot = rot @ rot_ro_cam\n",
    "\n",
    "    # # Apply position correction\n",
    "    # pos[1] += cam_height\n",
    "\n",
    "    # Create the pose matrix\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3, :3] = rot\n",
    "    pose_matrix[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    # Transform the points to global frame\n",
    "    points3d_homo = np.vstack([points3d_cam, np.ones((1, points3d_cam.shape[1]))])\n",
    "    points3d_global_homo = pose_matrix @ points3d_homo\n",
    "    points3d_global = points3d_global_homo[:3, :]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points3d_global.T)\n",
    "\n",
    "    # Assign color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.tile(color, (points3d_global.shape[1], 1)))\n",
    "\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_DBSCAN(point_cloud_o3d, eps=0.2, min_samples=20):\n",
    "\n",
    "    if point_cloud_o3d.is_empty():\n",
    "        return point_cloud_o3d\n",
    "\n",
    "    # Convert Open3D point cloud to NumPy arrays\n",
    "    points_np = np.asarray(point_cloud_o3d.points)\n",
    "    colors_np = np.asarray(point_cloud_o3d.colors)\n",
    "\n",
    "    # Convert NumPy array to CuPy array for GPU computations\n",
    "    points_gpu = cp.asarray(points_np)\n",
    "\n",
    "    # Create a DBSCAN instance with cuML\n",
    "    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Fit the model to the GPU data\n",
    "    dbscan_model.fit(points_gpu)\n",
    "\n",
    "    # Get the labels for the clusters\n",
    "    labels_gpu = dbscan_model.labels_\n",
    "\n",
    "    # Convert the labels back to a NumPy array\n",
    "    labels = cp.asnumpy(labels_gpu)\n",
    "\n",
    "    # Count the occurrence of each label to find the largest cluster\n",
    "    label_counter = Counter(labels)\n",
    "    label_counter.pop(-1, None)  # Remove the noise label (-1)\n",
    "    if not label_counter:  # If all points are noise, return an empty point cloud\n",
    "        return o3d.geometry.PointCloud()\n",
    "\n",
    "    # Find the label of the largest cluster\n",
    "    largest_cluster_label = max(label_counter, key=label_counter.get)\n",
    "\n",
    "    # Filter the points and colors that belong to the largest cluster\n",
    "    largest_cluster_points = points_np[labels == largest_cluster_label]\n",
    "    largest_cluster_colors = colors_np[labels == largest_cluster_label]\n",
    "\n",
    "    # Create a new Open3D point cloud with the points and colors of the largest cluster\n",
    "    largest_cluster_point_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    largest_cluster_point_cloud_o3d.points = o3d.utility.Vector3dVector(largest_cluster_points)\n",
    "    largest_cluster_point_cloud_o3d.colors = o3d.utility.Vector3dVector(largest_cluster_colors)\n",
    "\n",
    "    return largest_cluster_point_cloud_o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cosine_similarity(vec1, vec2):\n",
    "    # Ensure the vectors have the same shape\n",
    "    if vec1.shape != vec2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "\n",
    "    # Compute the dot product of the vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Compute the magnitudes (Euclidean norms) of the vectors\n",
    "    magnitude_vec1 = np.linalg.norm(vec1)\n",
    "    magnitude_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "    # Normalize the similarity value to [0, 1]\n",
    "    normalized_similarity = 0.5 * (similarity + 1)\n",
    "\n",
    "    return normalized_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Open3D point cloud to NumPy array\n",
    "def pointcloud_to_numpy(pcd):\n",
    "    return np.asarray(pcd.points)\n",
    "    # return np.asarray(pcd.points), np.asarray(pcd.colors)\n",
    "\n",
    "# Function to convert NumPy array to Open3D point cloud\n",
    "def numpy_to_pointcloud(points):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def get_text_clip_embedding(text):\n",
    "    text_inputs = clip.tokenize([text]).to(device)\n",
    "    \n",
    "    # Get the text features\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        \n",
    "    # Normalize the features\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return text_features.cpu().squeeze().numpy()\n",
    "\n",
    "ceiling_embed = get_text_clip_embedding(\"This is an image of a ceiling\")\n",
    "wall_embed = get_text_clip_embedding(\"This is an image of a wall\")\n",
    "floor_embed = get_text_clip_embedding(\"This is an image of floor\")\n",
    "chair_embed = get_text_clip_embedding(\"This is an image of a chair\")\n",
    "background_embed = get_text_clip_embedding(\"This is an image of a ceiling or wall or floor or pillar\")\n",
    "\n",
    "del model\n",
    "del transform\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obj Nodes Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(clip_embed, node_clip_embed):\n",
    "    return custom_cosine_similarity(clip_embed, node_clip_embed)\n",
    "\n",
    "def nnratio(pcd, node_pcd, delta_nn):\n",
    "    # Convert the point clouds into numpy arrays\n",
    "    point_array_pcd = np.asarray(pcd.points, dtype=np.float32)\n",
    "    point_array_node_pcd = np.asarray(node_pcd.points, dtype=np.float32)\n",
    "\n",
    "    # Create a FAISS index for the node_pcd for efficient search\n",
    "    index_node_pcd = faiss.IndexFlatL2(point_array_node_pcd.shape[1])\n",
    "    \n",
    "    # Add the points to the FAISS index\n",
    "    index_node_pcd.add(point_array_node_pcd)\n",
    "\n",
    "    # Search for the nearest neighbors of each point in pcd within node_pcd\n",
    "    D, I = index_node_pcd.search(point_array_pcd, 1)  # Search for the nearest neighbor\n",
    "\n",
    "    # Count the number of points in pcd that have neighbors within the distance delta_nn\n",
    "    count = (D < delta_nn ** 2).sum()\n",
    "\n",
    "    # Proportion of points within distance threshold\n",
    "    ratio = count / len(point_array_pcd)\n",
    "\n",
    "    return ratio\n",
    "\n",
    "\n",
    "def delta_sim(pcd, node_pcd, clip_embed, node_clip_embed, params):\n",
    "    if pcd.is_empty() or node_pcd.is_empty():\n",
    "        return 0\n",
    "\n",
    "    delta_geo = nnratio(pcd, node_pcd, params['delta_nn'])\n",
    "    delta_sem = cosine_sim(clip_embed, node_clip_embed)\n",
    "\n",
    "    delta_sim = delta_geo + delta_sem\n",
    "    \n",
    "    return delta_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nodes(node1, node2, params):\n",
    "    # Merge source IDs: source_ids: [(img_id, obj_id), ...]\n",
    "    source_ids = node1['source_ids'] + node2['source_ids']\n",
    "    count = len(source_ids)\n",
    "\n",
    "    # Average the embeddings\n",
    "    avg_clip_embed = (np.array(node1['clip_embed']) * len(node1['source_ids']) +\n",
    "                      np.array(node2['clip_embed']) * len(node2['source_ids'])) / count\n",
    "\n",
    "    avg_dino_embed = (np.array(node1['dino_embed']) * len(node1['source_ids']) +\n",
    "                      np.array(node2['dino_embed']) * len(node2['source_ids'])) / count\n",
    "\n",
    "    # Combine point clouds\n",
    "    merged_pcd = node1['pcd']\n",
    "    merged_pcd.points.extend(node2['pcd'].points)\n",
    "    merged_pcd = merged_pcd.voxel_down_sample(voxel_size=params['voxel_size'])\n",
    "\n",
    "    # make all points the same color (node1's color)\n",
    "    merged_pcd.colors = o3d.utility.Vector3dVector(np.tile(node1['pcd'].colors[0], (len(merged_pcd.points), 1)))\n",
    "\n",
    "    # Concatenate the points contributions from both nodes\n",
    "    points_contri = node1['points_contri'] + node2['points_contri']\n",
    "\n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'clip_embed': avg_clip_embed,\n",
    "        'dino_embed': avg_dino_embed,\n",
    "        'pcd': merged_pcd,\n",
    "        'points_contri': points_contri\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_scene_node_id(similarities, scene_obj_nodes, params):\n",
    "    # Find the node with the minimum similarity value, greedy assignment\n",
    "    max_sim_node_id = max(similarities, key=similarities.get)\n",
    "\n",
    "    # Check if the similarity is below the threshold\n",
    "    if similarities[max_sim_node_id] >= params['sim_thresh']:\n",
    "        return max_sim_node_id\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_words = ['ceiling', 'wall', 'floor', 'pillar', 'door', 'basement', 'room', 'workshop', 'warehouse']\n",
    "background_phrase = ['office']\n",
    "\n",
    "def check_background(obj_data):\n",
    "    obj_phrase = obj_data['phrase']\n",
    "    if obj_phrase in background_phrase:\n",
    "        return True\n",
    "\n",
    "    obj_words = obj_phrase.split()\n",
    "    for word in obj_words:\n",
    "        if word in background_words:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_scene_nodes(img_dict, params):\n",
    "    # Initialize an empty dictionary to store scene object nodes\n",
    "    scene_obj_nodes = {}\n",
    "\n",
    "    # Retrieve the initial image data using the provided ID\n",
    "    img_data = img_dict[params['init_img_id']]\n",
    "    img_path = img_data['img_path']\n",
    "    img_id = img_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image from BGR to RGB format\n",
    "\n",
    "    # Retrieve objects present in the image\n",
    "    objs = img_data['objs']\n",
    "    node_count = 1\n",
    "\n",
    "    for obj_id in objs:\n",
    "        obj_data = objs[obj_id]\n",
    "\n",
    "        # Calculate similarities\n",
    "        check_background_flag = check_background(obj_data)\n",
    "\n",
    "        if check_background_flag:\n",
    "            node_id = 0\n",
    "        else:\n",
    "            node_id = node_count\n",
    "            node_count += 1\n",
    "        \n",
    "        color = generate_pastel_color()\n",
    "        # Create a point cloud for the object\n",
    "        pcd = create_point_cloud(img_id, obj_data, params['cam_mat'], color=color)\n",
    "        pcd = pcd.voxel_down_sample(voxel_size=params['voxel_size'])\n",
    "\n",
    "        if node_id != 0:\n",
    "            pcd = fast_DBSCAN(pcd, eps=params['eps'], min_samples=params['min_samples'])\n",
    "\n",
    "        if pcd.is_empty():\n",
    "            continue\n",
    "        \n",
    "        if node_id not in scene_obj_nodes:\n",
    "            # Store the object data in the scene object nodes dictionary\n",
    "            scene_obj_nodes[node_id] = {'source_ids': [(params['init_img_id'], obj_id)], \n",
    "                                        'clip_embed': objs[obj_id]['clip_embed'], \n",
    "                                        'dino_embed': objs[obj_id]['dino_embed'], \n",
    "                                        'pcd': pcd, \n",
    "                                        'points_contri': [len(pcd.points)]}  # Count of points in the point cloud\n",
    "        else:\n",
    "            # Merge the object with the existing node\n",
    "            scene_obj_nodes[node_id] = merge_nodes(scene_obj_nodes[node_id], \n",
    "                                                    {'source_ids': [(params['init_img_id'], obj_id)], \n",
    "                                                    'clip_embed': objs[obj_id]['clip_embed'], \n",
    "                                                    'dino_embed': objs[obj_id]['dino_embed'], \n",
    "                                                    'pcd': pcd, \n",
    "                                                    'points_contri': [len(pcd.points)]}, params)\n",
    "\n",
    "    return scene_obj_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_object(img_id, obj_data, scene_obj_nodes, params):\n",
    "\n",
    "    color = generate_pastel_color()\n",
    "\n",
    "    check_background_flag = check_background(obj_data)\n",
    "\n",
    "    # Create a point cloud for the object\n",
    "    pcd = create_point_cloud(img_id, obj_data, params['cam_mat'], color=color)\n",
    "    pcd = pcd.voxel_down_sample(voxel_size=params['voxel_size'])\n",
    "\n",
    "    if check_background_flag is False:\n",
    "        pcd = fast_DBSCAN(pcd, eps=params['eps'], min_samples=params['min_samples'])\n",
    "\n",
    "    if pcd.is_empty():\n",
    "        return None, None\n",
    "    \n",
    "    # Compute similarities between the object and all nodes in the scene\n",
    "    similarities = {}\n",
    "\n",
    "    if check_background_flag:\n",
    "        # Manually set the similarity for node 0 as 2, and for all other nodes as 0\n",
    "        for key in scene_obj_nodes.keys():\n",
    "            similarities[key] = 2 if key == 0 else 0\n",
    "    else:\n",
    "\n",
    "        for node_id, node_data in scene_obj_nodes.items():\n",
    "            similarities[node_id] = delta_sim(pcd, node_data['pcd'], obj_data['clip_embed'], node_data['clip_embed'], params)\n",
    "\n",
    "    return similarities, pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene_nodes(img_id, img_data, scene_obj_nodes, params):\n",
    "\n",
    "    for obj_id, obj_data in img_data['objs'].items():\n",
    "        similarities, obj_pcd = process_object(img_id, obj_data, scene_obj_nodes, params)\n",
    "\n",
    "        if similarities is None:\n",
    "            continue\n",
    "        \n",
    "        # Determine whether to merge the object with an existing node or create a new one (id or None)\n",
    "        merge_scene_node_id = get_merge_scene_node_id(similarities, scene_obj_nodes, params)\n",
    "\n",
    "        if merge_scene_node_id is not None:\n",
    "            new_node = {\n",
    "                'source_ids': [(img_id, obj_id)],\n",
    "                'clip_embed': img_data['objs'][obj_id]['clip_embed'],\n",
    "                'dino_embed': img_data['objs'][obj_id]['dino_embed'],\n",
    "                'pcd': obj_pcd,\n",
    "                'points_contri': [len(obj_pcd.points)]}\n",
    "            \n",
    "            scene_node = scene_obj_nodes[merge_scene_node_id]\n",
    "            \n",
    "            scene_obj_nodes[merge_scene_node_id] = merge_nodes(scene_node, new_node, params)\n",
    "        else:\n",
    "            new_node_id = (max([int(i) for i in scene_obj_nodes.keys()]) + 1)\n",
    "            scene_obj_nodes[new_node_id] = {\n",
    "                'source_ids': [(img_id, obj_id)],\n",
    "                'clip_embed': img_data['objs'][obj_id]['clip_embed'],\n",
    "                'dino_embed': img_data['objs'][obj_id]['dino_embed'],\n",
    "                'pcd': obj_pcd,\n",
    "                'points_contri': [len(obj_pcd.points)]\n",
    "            }\n",
    "\n",
    "    return scene_obj_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_similarity(node1_id, node1_data, node2_id, node2_data, params):\n",
    "    # Computes the similarity between two nodes.\n",
    "    similarity = delta_sim(node1_data['pcd'], node2_data['pcd'], \n",
    "                           node1_data['clip_embed'], node2_data['clip_embed'], params)\n",
    "    return node1_id, node2_id, similarity\n",
    "\n",
    "def merge_similar_nodes(scene_obj_nodes, params):\n",
    "    # Merges nodes in the scene that have high similarity.\n",
    "    nodes_to_remove = set()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        \n",
    "        # Create a list of node pairs to check\n",
    "        node_ids = list(scene_obj_nodes.keys())\n",
    "        # remove node_id 0 from the list as it has ceiling, wall, floor\n",
    "        node_ids.remove(0)\n",
    "        \n",
    "        for i in range(len(node_ids)):\n",
    "            for j in range(i+1, len(node_ids)):\n",
    "                node1_id = node_ids[i]\n",
    "                node2_id = node_ids[j]\n",
    "\n",
    "                # Avoid re-checking nodes that have already been merged\n",
    "                if node1_id in nodes_to_remove or node2_id in nodes_to_remove:\n",
    "                    continue\n",
    "\n",
    "                future = executor.submit(compute_node_similarity, node1_id, scene_obj_nodes[node1_id],\n",
    "                                         node2_id, scene_obj_nodes[node2_id], params)\n",
    "                futures.append(future)\n",
    "        \n",
    "        # Process the computed similarities and merge nodes if needed\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            node1_id, node2_id, similarity = future.result()\n",
    "            \n",
    "            if similarity >= params['sim_thresh']:\n",
    "                merged_node = merge_nodes(scene_obj_nodes[node1_id], scene_obj_nodes[node2_id], params)\n",
    "                scene_obj_nodes[node1_id] = merged_node\n",
    "                \n",
    "                # Mark the second node for removal after merging\n",
    "                nodes_to_remove.add(node2_id)\n",
    "\n",
    "    # Remove nodes that were merged into other nodes\n",
    "    for node_id in nodes_to_remove:\n",
    "        del scene_obj_nodes[node_id]\n",
    "\n",
    "    return scene_obj_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'init_img_id': '585',\n",
    "          'sim_thresh': 1.1,\n",
    "          'voxel_size': 0.025,\n",
    "          'eps': 0.075,\n",
    "          'min_samples': 10,\n",
    "          'delta_nn': 0.025,\n",
    "          'cam_mat': get_kinect_cam_mat(),\n",
    "          'clip_sim_thresh': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the scene:  8\n"
     ]
    }
   ],
   "source": [
    "scene_obj_nodes = init_scene_nodes(img_dict, params)\n",
    "print(\"Number of nodes in the scene: \", len(scene_obj_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24173\n",
      "1 472\n",
      "2 449\n",
      "3 1396\n",
      "5 584\n",
      "6 325\n",
      "7 303\n",
      "8 261\n",
      "9 115\n"
     ]
    }
   ],
   "source": [
    "for node_id, node_data in scene_obj_nodes.items():\n",
    "    print(node_id, len(node_data['pcd'].points))\n",
    "    o3d.io.write_point_cloud(f'/scratch/kumaradi.gupta/kinect_pcds/{node_id}.pcd', node_data['pcd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dict_split = list(img_dict.items())[:50]\n",
    "img_dict_split = dict(img_dict_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3567af286245558a508fa2fa556f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter = 0\n",
    "for img_id, img_data in tqdm(img_dict.items()):\n",
    "    if len(img_data['objs']) == 0 or img_id == params['init_img_id']:\n",
    "        continue\n",
    "\n",
    "    scene_obj_nodes = update_scene_nodes(img_id, img_data, scene_obj_nodes, params)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 25 == 0:\n",
    "        scene_obj_nodes = merge_similar_nodes(scene_obj_nodes, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1 sources: [('243', 1)]\n",
      "office chair\n",
      "Node 2 sources: [('304', 2)]\n",
      "office chair swivel chair\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9320339316361556\n"
     ]
    }
   ],
   "source": [
    "node_id_1 = 53\n",
    "node_id_2 = 71\n",
    "\n",
    "print(f\"Node 1 sources: {scene_obj_nodes[node_id_1]['source_ids']}\")\n",
    "pcd1_img_id, pcd1_obj_id = scene_obj_nodes[node_id_1]['source_ids'][0]\n",
    "print(img_dict[pcd1_img_id]['objs'][pcd1_obj_id]['phrase'])\n",
    "\n",
    "print(f\"Node 2 sources: {scene_obj_nodes[node_id_2]['source_ids']}\")\n",
    "pcd2_img_id, pcd1_obj_id = scene_obj_nodes[node_id_2]['source_ids'][0]\n",
    "print(img_dict[pcd2_img_id]['objs'][pcd1_obj_id]['phrase'])\n",
    "\n",
    "print(delta_sim(scene_obj_nodes[node_id_1]['pcd'], \n",
    "          scene_obj_nodes[node_id_2]['pcd'], \n",
    "          scene_obj_nodes[node_id_1]['clip_embed'], \n",
    "          scene_obj_nodes[node_id_2]['clip_embed'], \n",
    "          params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objs in the scene:  239\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of objs in the scene: \", len(scene_obj_nodes))\n",
    "for node_id, node_data in scene_obj_nodes.items():\n",
    "    o3d.io.write_point_cloud(f'/scratch/kumaradi.gupta/kinect_pcds/{node_id}.pcd', node_data['pcd'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
