{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import open3d as o3d\n",
    "from cuml.cluster import DBSCAN\n",
    "import cupy as cp\n",
    "\n",
    "import clip\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iou import compute_3d_iou_accuracte_batch, compute_3d_iou, compute_iou_batch\n",
    "from vis import visualize_and_capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"/scratch/kumaradi.gupta/checkpoints\"\n",
    "\n",
    "imgs_dir = \"/scratch/kumaradi.gupta/run_kinect_wheel_1/rgb\"\n",
    "depth_dir = \"/scratch/kumaradi.gupta/run_kinect_wheel_1/depth/\"\n",
    "pose_dir = \"/scratch/kumaradi.gupta/run_kinect_wheel_1/pose/\"\n",
    "\n",
    "# imgs_dir = \"/scratch/kumaradi.gupta/KM_pipeline_rtab/rgb/\"\n",
    "# depth_dir = \"/scratch/kumaradi.gupta/KM_pipeline_rtab/depth/\"\n",
    "# pose_dir = \"/scratch/kumaradi.gupta/KM_pipeline_rtab/pose/\"\n",
    "\n",
    "img_dict_dir = \"/scratch/kumaradi.gupta/kinect_img_dict.pkl\"\n",
    "# img_dict_dir = \"/scratch/kumaradi.gupta/handheld_img_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pastel_color():\n",
    "    # generate (r, g, b) tuple of random numbers between 0.5 and 1, truncate to 2 decimal places\n",
    "    r = round(random.uniform(0.5, 1), 2)\n",
    "    g = round(random.uniform(0.5, 1), 2)\n",
    "    b = round(random.uniform(0.5, 1), 2)\n",
    "    return (r, g, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle file\n",
    "with open(img_dict_dir, 'rb') as file:\n",
    "    img_dict = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "img_dict = {img_name: {img_path: str,\n",
    "                        ram_tags: list_of_str,\n",
    "                        objs: {0: {bbox: [x1, y1, x2, y2],\n",
    "                                    phrase: str,\n",
    "                                    clip_embed: [1, 1024]},\n",
    "                                    dino_embed: [1, 1024]},\n",
    "                                    mask: [h, w],\n",
    "                                    prob: float,\n",
    "                                    aabb: arr}\n",
    "                                1: {...},\n",
    "                        }\n",
    "            img_name: {...},\n",
    "            }\n",
    "'''\n",
    "\n",
    "def get_depth(img_name):\n",
    "    # depth_path = os.path.join(depth_dir, img_name + '.npy')\n",
    "    # depth = np.load(depth_path)\n",
    "\n",
    "    depth_path = os.path.join(depth_dir, img_name + '.png')\n",
    "    depth = cv2.imread(depth_path, cv2.IMREAD_ANYDEPTH)\n",
    "    depth = depth.astype(np.float32) / 1000.0\n",
    "    return depth\n",
    "\n",
    "def get_pose(img_name):\n",
    "    pose_path = os.path.join(pose_dir, img_name + '.txt')\n",
    "\n",
    "    # check if the pose file exists, if it doesn't, return None\n",
    "    # [x, y, z, qx, qy, qz, qw]\n",
    "    if not os.path.exists(pose_path):\n",
    "        return None\n",
    "    \n",
    "    with open(pose_path, 'r') as f:\n",
    "        pose = f.read().split()\n",
    "        pose = np.array(pose).astype(np.float32)\n",
    "    return pose\n",
    "\n",
    "def get_sim_cam_mat_with_fov(h, w, fov):\n",
    "    cam_mat = np.eye(3)\n",
    "    cam_mat[0, 0] = cam_mat[1, 1] = w / (2.0 * np.tan(np.deg2rad(fov / 2)))\n",
    "    cam_mat[0, 2] = w / 2.0\n",
    "    cam_mat[1, 2] = h / 2.0\n",
    "    return cam_mat\n",
    "\n",
    "def get_realsense_cam_mat():\n",
    "    K = np.array([[386.458, 0, 321.111],\n",
    "              [0, 386.458, 241.595],\n",
    "              [0, 0, 1]])\n",
    "    return K\n",
    "\n",
    "def get_kinect_cam_mat():\n",
    "    K = np.array([[9.7096624755859375e+02, 0., 1.0272059326171875e+03], \n",
    "                  [0., 9.7109600830078125e+02, 7.7529718017578125e+02], \n",
    "                  [0., 0., 1]])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_point_cloud(img_id, obj_data, cam_mat, color=(1, 0, 0), cam_height=0.9):\n",
    "    \"\"\"\n",
    "    Generates a point cloud from a depth image, camera intrinsics, mask, and pose.\n",
    "    Only points within the mask and with valid depth are added to the cloud.\n",
    "    Points are colored using the specified color.\n",
    "    \"\"\"\n",
    "    \n",
    "    depth = get_depth(img_id)\n",
    "    pose = get_pose(img_id)\n",
    "    mask = obj_data['mask']\n",
    "\n",
    "    if pose is None:\n",
    "        return o3d.geometry.PointCloud()\n",
    "\n",
    "    # Reproject the depth to 3D space\n",
    "    rows, cols = np.where(mask)\n",
    "\n",
    "    depth_values = depth[rows, cols]\n",
    "    valid_depth_indices = (depth_values > 0) & (depth_values <= 5)\n",
    "\n",
    "    rows = rows[valid_depth_indices]\n",
    "    cols = cols[valid_depth_indices]\n",
    "    depth_values = depth_values[valid_depth_indices]\n",
    "\n",
    "    points2d = np.vstack([cols, rows, np.ones_like(rows)])\n",
    "\n",
    "    cam_mat_inv = np.linalg.inv(cam_mat)\n",
    "    points3d_cam = cam_mat_inv @ points2d * depth_values\n",
    "\n",
    "    # Parse the pose\n",
    "    pos = np.array(pose[:3], dtype=float).reshape((3, 1))\n",
    "    quat = pose[3:]\n",
    "    rot = R.from_quat(quat).as_matrix()\n",
    "\n",
    "    # # Apply rotation correction, to match the orientation z: backward, y: upward, and x: right\n",
    "    # rot_ro_cam = np.eye(3)\n",
    "    # rot_ro_cam[1, 1] = -1\n",
    "    # rot_ro_cam[2, 2] = -1\n",
    "    # rot = rot @ rot_ro_cam\n",
    "\n",
    "    # # Apply position correction\n",
    "    # pos[1] += cam_height\n",
    "\n",
    "    # Create the pose matrix\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3, :3] = rot\n",
    "    pose_matrix[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    # Transform the points to global frame\n",
    "    points3d_homo = np.vstack([points3d_cam, np.ones((1, points3d_cam.shape[1]))])\n",
    "    points3d_global_homo = pose_matrix @ points3d_homo\n",
    "    points3d_global = points3d_global_homo[:3, :]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points3d_global.T)\n",
    "\n",
    "    # Assign color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.tile(color, (points3d_global.shape[1], 1)))\n",
    "\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_DBSCAN(point_cloud_o3d, eps=0.2, min_samples=20):\n",
    "\n",
    "    if point_cloud_o3d.is_empty():\n",
    "        return point_cloud_o3d\n",
    "\n",
    "    # Convert Open3D point cloud to NumPy arrays\n",
    "    points_np = np.asarray(point_cloud_o3d.points)\n",
    "    colors_np = np.asarray(point_cloud_o3d.colors)\n",
    "\n",
    "    # Convert NumPy array to CuPy array for GPU computations\n",
    "    points_gpu = cp.asarray(points_np)\n",
    "\n",
    "    # Create a DBSCAN instance with cuML\n",
    "    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Fit the model to the GPU data\n",
    "    dbscan_model.fit(points_gpu)\n",
    "\n",
    "    # Get the labels for the clusters\n",
    "    labels_gpu = dbscan_model.labels_\n",
    "\n",
    "    # Convert the labels back to a NumPy array\n",
    "    labels = cp.asnumpy(labels_gpu)\n",
    "\n",
    "    # Count the occurrence of each label to find the largest cluster\n",
    "    label_counter = Counter(labels)\n",
    "    label_counter.pop(-1, None)  # Remove the noise label (-1)\n",
    "    if not label_counter:  # If all points are noise, return an empty point cloud\n",
    "        return o3d.geometry.PointCloud()\n",
    "\n",
    "    # Find the label of the largest cluster\n",
    "    largest_cluster_label = max(label_counter, key=label_counter.get)\n",
    "\n",
    "    # Filter the points and colors that belong to the largest cluster\n",
    "    largest_cluster_points = points_np[labels == largest_cluster_label]\n",
    "    largest_cluster_colors = colors_np[labels == largest_cluster_label]\n",
    "\n",
    "    # Create a new Open3D point cloud with the points and colors of the largest cluster\n",
    "    largest_cluster_point_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    largest_cluster_point_cloud_o3d.points = o3d.utility.Vector3dVector(largest_cluster_points)\n",
    "    largest_cluster_point_cloud_o3d.colors = o3d.utility.Vector3dVector(largest_cluster_colors)\n",
    "\n",
    "    return largest_cluster_point_cloud_o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cosine_similarity(vec1, vec2):\n",
    "    # Ensure the vectors have the same shape\n",
    "    if vec1.shape != vec2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "\n",
    "    # Compute the dot product of the vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Compute the magnitudes (Euclidean norms) of the vectors\n",
    "    magnitude_vec1 = np.linalg.norm(vec1)\n",
    "    magnitude_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "    # Normalize the similarity value to [0, 1]\n",
    "    normalized_similarity = 0.5 * (similarity + 1)\n",
    "\n",
    "    return normalized_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Open3D point cloud to NumPy array\n",
    "def pointcloud_to_numpy(pcd):\n",
    "    return np.asarray(pcd.points)\n",
    "    # return np.asarray(pcd.points), np.asarray(pcd.colors)\n",
    "\n",
    "# Function to convert NumPy array to Open3D point cloud\n",
    "def numpy_to_pointcloud(points):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_icp(source, target, params):\n",
    "    # Set ICP configuration\n",
    "    config = o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                            relative_rmse=1e-6, max_iteration=params['icp_max_iter'])\n",
    "    \n",
    "    icp_threshold = params['voxel_size'] * params['icp_threshold_multiplier']\n",
    "\n",
    "    # Run ICP\n",
    "    result_icp = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, icp_threshold, np.eye(4),\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "        config)\n",
    "    \n",
    "    # Update pcd based on the transformation matrix obtained from ICP\n",
    "    source.transform(result_icp.transformation)\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def get_text_clip_embedding(text):\n",
    "    text_inputs = clip.tokenize([text]).to(device)\n",
    "    \n",
    "    # Get the text features\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        \n",
    "    # Normalize the features\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return text_features.cpu().squeeze().numpy()\n",
    "\n",
    "ceiling_embed = get_text_clip_embedding(\"This is an image of a ceiling\")\n",
    "wall_embed = get_text_clip_embedding(\"This is an image of a wall\")\n",
    "floor_embed = get_text_clip_embedding(\"This is an image of floor\")\n",
    "chair_embed = get_text_clip_embedding(\"This is an image of a chair\")\n",
    "background_embed = get_text_clip_embedding(\"This is an image of a ceiling or wall or floor or pillar\")\n",
    "\n",
    "del model\n",
    "del transform\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obj Nodes Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pcd(pcd, params, run_dbscan=True):\n",
    "    pcd = pcd.voxel_down_sample(voxel_size=params['voxel_size'])\n",
    "\n",
    "    if run_dbscan:\n",
    "        pcd = fast_DBSCAN(pcd, eps=params['eps'], min_samples=params['min_samples'])\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "def get_bounding_box(pcd, params):\n",
    "    try:\n",
    "        return pcd.get_oriented_bounding_box(robust=True)\n",
    "    except RuntimeError as e:\n",
    "        # print(f\"Met {e}, use axis aligned bounding box instead\")\n",
    "        return pcd.get_axis_aligned_bounding_box()\n",
    "\n",
    "def check_background(obj_data):\n",
    "    background_words = ['ceiling', 'wall', 'floor', 'pillar', 'door', 'basement', 'room', 'workshop', 'warehouse']\n",
    "    background_phrase = ['office']\n",
    "    \n",
    "    obj_phrase = obj_data['phrase']\n",
    "    if obj_phrase in background_phrase:\n",
    "        return True\n",
    "\n",
    "    obj_words = obj_phrase.split()\n",
    "    for word in obj_words:\n",
    "        if word in background_words:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nodes(node1, node2, params, run_icp=False):\n",
    "    # Merge source IDs: source_ids: [(img_id, obj_id), ...]\n",
    "    source_ids = node1['source_ids'] + node2['source_ids']\n",
    "    count = len(source_ids)\n",
    "\n",
    "    # Average the embeddings\n",
    "    avg_clip_embed = (np.array(node1['clip_embed']) * len(node1['source_ids']) +\n",
    "                      np.array(node2['clip_embed']) * len(node2['source_ids'])) / count\n",
    "\n",
    "    avg_dino_embed = (np.array(node1['dino_embed']) * len(node1['source_ids']) +\n",
    "                      np.array(node2['dino_embed']) * len(node2['source_ids'])) / count\n",
    "    \n",
    "    if run_icp:\n",
    "        node2['pcd'] = vanilla_icp(node2['pcd'], node1['pcd'], params)\n",
    "\n",
    "    # Combine point clouds\n",
    "    merged_pcd = node1['pcd']\n",
    "    merged_pcd.points.extend(node2['pcd'].points)\n",
    "\n",
    "    # make all points the same color (node1's color)\n",
    "    merged_pcd.colors = o3d.utility.Vector3dVector(np.tile(node1['pcd'].colors[0], (len(merged_pcd.points), 1)))\n",
    "    merged_pcd = process_pcd(merged_pcd, params)\n",
    "\n",
    "    bbox = get_bounding_box(merged_pcd, params)\n",
    "\n",
    "    # Concatenate the points contributions from both nodes\n",
    "    points_contri = node1['points_contri'] + node2['points_contri']\n",
    "\n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'clip_embed': avg_clip_embed,\n",
    "        'dino_embed': avg_dino_embed,\n",
    "        'pcd': merged_pcd,\n",
    "        'bbox': bbox,\n",
    "        'points_contri': points_contri\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_scene_nodes(img_data, img_id, params):\n",
    "    # Initialize an empty dictionary to store scene object nodes\n",
    "    scene_obj_nodes = {}\n",
    "\n",
    "    # Retrieve the initial image data using the provided ID\n",
    "    img_path = img_data['img_path']\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image from BGR to RGB format\n",
    "\n",
    "    # Retrieve objects present in the image\n",
    "    objs = img_data['objs']\n",
    "    node_count = 1\n",
    "\n",
    "    for obj_id in objs.keys():\n",
    "        obj_data = objs[obj_id]\n",
    "\n",
    "        # Calculate similarities\n",
    "        check_background_flag = check_background(obj_data)\n",
    "\n",
    "        if check_background_flag:\n",
    "            node_id = 0\n",
    "            continue # Skipping the background objects for now\n",
    "        else:\n",
    "            node_id = node_count\n",
    "            node_count += 1\n",
    "        \n",
    "        color = generate_pastel_color()\n",
    "        pcd = create_point_cloud(img_id, obj_data, params['cam_mat'], color=color)\n",
    "        pcd = process_pcd(pcd, params)\n",
    "\n",
    "        bbox = get_bounding_box(pcd, params)\n",
    "        if bbox.volume() < 1e-6 or len(pcd.points) < 10:\n",
    "            continue\n",
    "        \n",
    "        if node_id not in scene_obj_nodes:\n",
    "            # Store the object data in the scene object nodes dictionary\n",
    "            scene_obj_nodes[node_id] = {'source_ids': [(img_id, obj_id)], \n",
    "                                        'clip_embed': objs[obj_id]['clip_embed'], \n",
    "                                        'dino_embed': objs[obj_id]['dino_embed'], \n",
    "                                        'pcd': pcd, \n",
    "                                        'bbox': bbox,\n",
    "                                        'points_contri': [len(pcd.points)]}  # Count of points in the point cloud\n",
    "        else:\n",
    "            # Merge the object with the existing node\n",
    "            scene_obj_nodes[node_id] = merge_nodes(scene_obj_nodes[node_id], \n",
    "                                                    {'source_ids': [(img_id, obj_id)], \n",
    "                                                    'clip_embed': objs[obj_id]['clip_embed'], \n",
    "                                                    'dino_embed': objs[obj_id]['dino_embed'], \n",
    "                                                    'pcd': pcd,\n",
    "                                                    'bbox': bbox, \n",
    "                                                    'points_contri': [len(pcd.points)]}, params)\n",
    "\n",
    "\n",
    "    # print(\"Number of nodes in the scene: \", len(scene_obj_nodes))\n",
    "    return scene_obj_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap_matrix_2set(scene_obj_nodes, det_nodes, params):\n",
    "    '''\n",
    "    Compute pairwise overlapping between two sets of objects in terms of point nearest neighbor.\n",
    "    scene_obj_nodes is the existing objects in the scene, det_nodes is the new objects to be added to the scene\n",
    "    '''\n",
    "\n",
    "    m = len(scene_obj_nodes)\n",
    "    n = len(det_nodes)\n",
    "    overlap_matrix = np.zeros((m, n))\n",
    "\n",
    "    # Convert the point clouds into numpy arrays and then into FAISS indices for efficient search\n",
    "    points_map = [np.asarray(obj['pcd'].points, dtype=np.float32) for obj in scene_obj_nodes.values()]\n",
    "    indices = [faiss.IndexFlatL2(arr.shape[1]) for arr in points_map] # m indices\n",
    "    \n",
    "    # Add the points from the numpy arrays to the corresponding FAISS indices\n",
    "    for index, arr in zip(indices, points_map):\n",
    "        index.add(arr)\n",
    "        \n",
    "    points_new = [np.asarray(obj['pcd'].points, dtype=np.float32) for obj in det_nodes.values()]\n",
    "    \n",
    "    # Assuming you can compute 3D IoU given the 'bbox' field in your dicts\n",
    "    bbox_map_np = np.array([obj['bbox'].get_box_points() for obj in scene_obj_nodes.values()])\n",
    "    bbox_map = torch.from_numpy(bbox_map_np).to(device)\n",
    "\n",
    "    bbox_new_np = np.array([obj['bbox'].get_box_points() for obj in det_nodes.values()])\n",
    "    bbox_new = torch.from_numpy(bbox_new_np).to(device)\n",
    "    \n",
    "    try:\n",
    "        # Assuming you have a function called compute_3d_iou_accurate_batch that takes PyTorch tensors\n",
    "        iou = compute_3d_iou_accuracte_batch(bbox_map, bbox_new)  # (m, n)\n",
    "    except ValueError:\n",
    "        # If you encounter the \"Plane vertices are not coplanar\" error, switch to axis-aligned bounding boxes\n",
    "        bbox_map = []\n",
    "        bbox_new = []\n",
    "        \n",
    "        for node in scene_obj_nodes.values():\n",
    "            bbox_map.append(np.asarray(\n",
    "                node['pcd'].get_axis_aligned_bounding_box().get_box_points()\n",
    "            ))\n",
    "\n",
    "        for node in det_nodes.values():\n",
    "            bbox_new.append(np.asarray(\n",
    "                node['pcd'].get_axis_aligned_bounding_box().get_box_points()\n",
    "            ))\n",
    "        \n",
    "        bbox_map = torch.tensor(np.stack(bbox_map))\n",
    "        bbox_new = torch.tensor(np.stack(bbox_new))\n",
    "        \n",
    "        # Assuming you have a function called compute_iou_batch that takes PyTorch tensors\n",
    "        iou = compute_iou_batch(bbox_map, bbox_new)  # (m, n)\n",
    "\n",
    "\n",
    "    # Compute the pairwise overlaps\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if iou[i, j] < 1e-6:\n",
    "                continue\n",
    "            \n",
    "            D, I = indices[i].search(points_new[j], 1)  # Search new object j in map object i\n",
    "\n",
    "            overlap = (D < params['voxel_size'] ** 2).sum()  # D is the squared distance\n",
    "\n",
    "            # Calculate the ratio of points within the threshold\n",
    "            overlap_matrix[i, j] = overlap / len(points_new[j])\n",
    "\n",
    "    return overlap_matrix\n",
    "\n",
    "def compute_spatial_similarity(scene_obj_nodes, det_nodes, params):\n",
    "    spatial_sim = compute_overlap_matrix_2set(scene_obj_nodes, det_nodes, params)\n",
    "    spatial_sim = torch.from_numpy(spatial_sim).T\n",
    "    spatial_sim = spatial_sim.to(device)\n",
    "\n",
    "    return spatial_sim\n",
    "\n",
    "def compute_visual_similarity(scene_obj_nodes, det_nodes, params):\n",
    "    '''\n",
    "    Compute the visual similarities between the detections and the objects.\n",
    "    \n",
    "    Args:\n",
    "        scene_obj_nodes: a dict of N objects in the scene\n",
    "        det_nodes: a dict of M detections\n",
    "    Returns:\n",
    "        A MxN tensor of visual similarities\n",
    "    '''\n",
    "    \n",
    "    # Extract clip embeddings from scene_obj_nodes and det_nodes dictionaries\n",
    "    embed_type = params['embed_type']\n",
    "    obj_fts = np.array([obj[embed_type] for obj in scene_obj_nodes.values()]) # (N, D)\n",
    "    det_fts = np.array([obj[embed_type] for obj in det_nodes.values()]) # (M, D)\n",
    "\n",
    "    obj_fts = torch.from_numpy(obj_fts).to(device)\n",
    "    det_fts = torch.from_numpy(det_fts).to(device)\n",
    "    \n",
    "    # Reshape tensors to match dimensions for cosine similarity\n",
    "    det_fts = det_fts.unsqueeze(-1)  # (M, D, 1)\n",
    "    obj_fts = obj_fts.T.unsqueeze(0)  # (1, D, N)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    visual_sim = F.cosine_similarity(det_fts, obj_fts, dim=1)  # (M, N)\n",
    "    \n",
    "    # Scale the visual similarity to be between 0 and 1\n",
    "    # scaled_visual_sim = (visual_sim + 1) / 2\n",
    "    \n",
    "    return visual_sim\n",
    "\n",
    "def compute_aggregate_similarity(scene_obj_nodes, det_nodes, params):\n",
    "    '''\n",
    "    Compute the aggregate similarity between the detections and the objects.\n",
    "    \n",
    "    Args:\n",
    "        scene_obj_nodes: a dict of N objects in the scene\n",
    "        det_nodes: a dict of M detections\n",
    "    Returns:\n",
    "        A MxN tensor of aggregate similarities\n",
    "    '''\n",
    "\n",
    "    spatial_sim = compute_spatial_similarity(scene_obj_nodes, det_nodes, params)\n",
    "    visual_sim  = compute_visual_similarity(scene_obj_nodes, det_nodes, params)\n",
    "    aggregate_sim = (1 + params['alpha']) * visual_sim + (1 - params['alpha']) * spatial_sim\n",
    "\n",
    "    # if value in row is less than threshold, set it to -inf\n",
    "    aggregate_sim[aggregate_sim < params['sim_threshold']] = -float('inf')\n",
    "\n",
    "    return aggregate_sim, spatial_sim, visual_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene_nodes(img_id, img_data, scene_obj_nodes, params):\n",
    "    det_nodes = init_scene_nodes(img_data, img_id, params)  # Assuming img_data should be used\n",
    "    \n",
    "    if len(det_nodes) == 0:\n",
    "        return scene_obj_nodes\n",
    "    \n",
    "    # Assuming you have a function named compute_aggregate_similarity to get aggregate similarity\n",
    "    aggregate_sim, _, _ = compute_aggregate_similarity(scene_obj_nodes, det_nodes, params)\n",
    "    \n",
    "    # Initialize a new dictionary to store updated scene_obj_nodes\n",
    "    updated_scene_obj_nodes = scene_obj_nodes.copy()\n",
    "    \n",
    "    # Find the maximum existing key in scene_obj_nodes\n",
    "    max_scene_key = max(scene_obj_nodes.keys(), default=0)\n",
    "    \n",
    "    # Iterate through all detected nodes to merge them into existing scene_obj_nodes\n",
    "    for i, det_key in enumerate(det_nodes.keys()):\n",
    "        # If not matched to any object in the scene, add it as a new object\n",
    "        if aggregate_sim[i].max() == float('-inf'):\n",
    "            new_key = max_scene_key + det_key  # Create a new unique key\n",
    "            updated_scene_obj_nodes[new_key] = det_nodes[det_key]\n",
    "        else:\n",
    "            # Merge with most similar existing object in the scene\n",
    "            j = aggregate_sim[i].argmax().item()\n",
    "            scene_key = list(scene_obj_nodes.keys())[j]\n",
    "            matched_det = det_nodes[det_key]\n",
    "            matched_obj = scene_obj_nodes[scene_key]\n",
    "            \n",
    "            # Merge the matched detection node into the matched scene node\n",
    "            merged_obj = merge_nodes(matched_obj, matched_det, params)\n",
    "            updated_scene_obj_nodes[scene_key] = merged_obj\n",
    "    \n",
    "    return updated_scene_obj_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scene_nodes(scene_obj_nodes, params):\n",
    "    print(\"Before filtering:\", len(scene_obj_nodes))\n",
    "    \n",
    "    # Initialize a new dictionary to store the filtered scene_obj_nodes\n",
    "    filtered_scene_obj_nodes = {}\n",
    "    \n",
    "    for key, obj in scene_obj_nodes.items():\n",
    "        # Use len(obj['pcd'].points) to get the number of points and len(obj['source_ids']) to get the number of views\n",
    "        if len(obj['pcd'].points) >= params['obj_min_points'] and len(obj['source_ids']) >= params['obj_min_detections']:\n",
    "            filtered_scene_obj_nodes[key] = obj\n",
    "\n",
    "    print(\"After filtering:\", len(filtered_scene_obj_nodes))\n",
    "    \n",
    "    return filtered_scene_obj_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap_matrix_nodes(params, scene_obj_nodes):\n",
    "    n = len(scene_obj_nodes)\n",
    "    overlap_matrix = np.zeros((n, n))\n",
    "    \n",
    "    point_arrays = [np.asarray(obj['pcd'].points, dtype=np.float32) for obj in scene_obj_nodes.values()]\n",
    "    indices = [faiss.IndexFlatL2(arr.shape[1]) for arr in point_arrays]\n",
    "    \n",
    "    for index, arr in zip(indices, point_arrays):\n",
    "        index.add(arr)\n",
    "    \n",
    "    for i, obj_i in enumerate(scene_obj_nodes.values()):\n",
    "        for j, obj_j in enumerate(scene_obj_nodes.values()):\n",
    "            if i != j:\n",
    "                box_i = obj_i['bbox']\n",
    "                box_j = obj_j['bbox']\n",
    "\n",
    "                if params['merge_overlap_method'] == 'iou':\n",
    "                    iou = compute_3d_iou(box_i, box_j)\n",
    "                    overlap_matrix[i, j] = iou\n",
    "\n",
    "                elif params['merge_overlap_method'] == 'max_overlap':\n",
    "                    iou = compute_3d_iou(box_i, box_j, use_iou=False)\n",
    "                    overlap_matrix[i, j] = iou\n",
    "                    \n",
    "                elif params['merge_overlap_method'] == 'nnratio':\n",
    "                    iou = compute_3d_iou(box_i, box_j)\n",
    "                    if iou == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    D, I = indices[j].search(point_arrays[i], 1)\n",
    "                    overlap = (D < params['voxel_size'] ** 2).sum()\n",
    "                    overlap_matrix[i, j] = overlap / len(point_arrays[i])\n",
    "    \n",
    "    return overlap_matrix\n",
    "\n",
    "def merge_overlap_nodes(params, scene_obj_nodes, overlap_matrix):\n",
    "    x, y = overlap_matrix.nonzero()\n",
    "    overlap_ratio = overlap_matrix[x, y]\n",
    "    \n",
    "    sort = np.argsort(overlap_ratio)[::-1]\n",
    "    x = x[sort]\n",
    "    y = y[sort]\n",
    "    overlap_ratio = overlap_ratio[sort]\n",
    "    \n",
    "    kept_keys = list(scene_obj_nodes.keys())\n",
    "    merged_keys = set()  # Keep track of keys that have been merged into others\n",
    "    \n",
    "    for i, j, ratio in zip(x, y, overlap_ratio):\n",
    "        key_i = kept_keys[i]\n",
    "        key_j = kept_keys[j]\n",
    "        \n",
    "        # Skip if these keys have been merged into others\n",
    "        if key_i in merged_keys or key_j in merged_keys:\n",
    "            continue\n",
    "\n",
    "        embed_type = params['embed_type']\n",
    "        visual_sim = F.cosine_similarity(\n",
    "            torch.tensor(scene_obj_nodes[key_i][embed_type]).to(device),\n",
    "            torch.tensor(scene_obj_nodes[key_j][embed_type]).to(device),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        overall_sim = (1 + params['alpha']) * visual_sim + (1 - params['alpha']) * ratio\n",
    "        \n",
    "        if overall_sim > params['merge_overall_thresh']:\n",
    "            if key_j in scene_obj_nodes: # Check if key_j still exists\n",
    "                scene_obj_nodes[key_j] = merge_nodes(scene_obj_nodes[key_j], scene_obj_nodes[key_i], params, run_icp=False)\n",
    "                del scene_obj_nodes[key_i]\n",
    "                merged_keys.add(key_i)  # Mark key_i as merged\n",
    "    \n",
    "    return scene_obj_nodes\n",
    "\n",
    "def merge_scene_nodes(scene_obj_nodes, params):\n",
    "    if params['merge_overlap_thresh'] > 0:\n",
    "        print(\"Before merging:\", len(scene_obj_nodes))\n",
    "        \n",
    "        # Compute the overlap matrix\n",
    "        overlap_matrix = compute_overlap_matrix_nodes(params, scene_obj_nodes)\n",
    "        \n",
    "        # Merge overlapping nodes\n",
    "        scene_obj_nodes = merge_overlap_nodes(params, scene_obj_nodes, overlap_matrix)\n",
    "        \n",
    "        print(\"After merging:\", len(scene_obj_nodes))\n",
    "    \n",
    "    return scene_obj_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'init_img_id': '1', #initialize the scene with this image\n",
    "          \n",
    "          'voxel_size': 0.025, #voxel size for downsampling point clouds\n",
    "          'eps': 0.075, #eps for DBSCAN\n",
    "          'min_samples': 10, #min_samples for DBSCAN\n",
    "          'embed_type': 'dino_embed', #embedding type to use for visual similarity\n",
    "          \n",
    "          'sim_threshold': 0.95, #threshold for aggregate similarity while running update_scene_nodes\n",
    "          'alpha': 0, #weight for visual similarity while computing aggregate similarity\n",
    "\n",
    "          'merge_overlap_method': 'nnratio', #metric to use for merging overlapping nodes\n",
    "          'merge_overlap_thresh': 0.95, #threshold for overlap ratio while merging nodes in scene\n",
    "          'merge_visual_thresh': 0.75, #threshold for visual similarity while merging nodes in scene\n",
    "          'merge_overall_thresh' : 0.95, #threshold for overall similarity while merging nodes in scene\n",
    "\n",
    "          'obj_min_points': 30, #minimum number of points in a node while filtering scene nodes\n",
    "          'obj_min_detections': 10, #minimum number of detections in a node while filtering scene nodes\n",
    "\n",
    "          'icp_threshold_multiplier': 1.5, #threshold multiplier for ICP\n",
    "          'icp_max_iter': 2000, #maximum number of iterations for ICP\n",
    "\n",
    "          'cam_mat': get_kinect_cam_mat(), #camera matrix\n",
    "\n",
    "          'img_size': (2048, 1536), #image size\n",
    "          'save_folder': '/scratch/kumaradi.gupta/kinect_output_imgs/'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_obj_nodes = init_scene_nodes(img_dict[params['init_img_id']], \n",
    "                                   params['init_img_id'], \n",
    "                                   params)\n",
    "\n",
    "# visualize_and_capture(params['init_img_id'], scene_obj_nodes, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "visualize_and_capture(params['init_img_id'], scene_obj_nodes, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_dict_split = list(img_dict.items())[:50]\n",
    "# img_dict_split = dict(img_dict_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7f58f471784c12b9a1dbbef4835969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging: 61\n",
      "After merging: 54\n",
      "Before merging: 102\n",
      "After merging: 91\n",
      "Before merging: 151\n",
      "After merging: 140\n",
      "Before merging: 177\n",
      "After merging: 168\n",
      "Before merging: 187\n",
      "After merging: 182\n",
      "Before merging: 200\n",
      "After merging: 192\n",
      "Before merging: 227\n",
      "After merging: 217\n",
      "Before merging: 239\n",
      "After merging: 225\n",
      "Before merging: 238\n",
      "After merging: 231\n",
      "Before merging: 242\n",
      "After merging: 234\n",
      "Before merging: 247\n",
      "After merging: 236\n",
      "Before merging: 254\n",
      "After merging: 248\n",
      "Before merging: 261\n",
      "After merging: 256\n",
      "Before merging: 263\n",
      "After merging: 261\n",
      "Before merging: 268\n",
      "After merging: 264\n",
      "Before merging: 278\n",
      "After merging: 272\n",
      "Before merging: 277\n",
      "After merging: 274\n",
      "Before merging: 279\n",
      "After merging: 272\n",
      "Before merging: 282\n",
      "After merging: 278\n",
      "Before merging: 287\n",
      "After merging: 280\n",
      "Before merging: 290\n",
      "After merging: 290\n",
      "Before merging: 292\n",
      "After merging: 287\n",
      "Before merging: 292\n",
      "After merging: 290\n",
      "Before merging: 299\n",
      "After merging: 295\n",
      "Before merging: 305\n",
      "After merging: 300\n",
      "Before merging: 307\n",
      "After merging: 305\n",
      "Before merging: 310\n",
      "After merging: 307\n",
      "Before merging: 312\n",
      "After merging: 309\n",
      "Before merging: 313\n",
      "After merging: 308\n",
      "Before merging: 313\n",
      "After merging: 310\n",
      "Before merging: 314\n",
      "After merging: 313\n",
      "Before merging: 316\n",
      "After merging: 314\n",
      "Before merging: 317\n",
      "After merging: 315\n",
      "Before merging: 323\n",
      "After merging: 322\n",
      "Before merging: 330\n",
      "After merging: 329\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for img_id, img_data in tqdm(img_dict.items()):\n",
    "    if len(img_data['objs']) == 0 or img_id == params['init_img_id']:\n",
    "        continue\n",
    "\n",
    "    scene_obj_nodes = update_scene_nodes(img_id, img_data, scene_obj_nodes, params)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 25 == 0:\n",
    "        scene_obj_nodes = merge_scene_nodes(scene_obj_nodes, params)\n",
    "\n",
    "    visualize_and_capture(img_id, scene_obj_nodes, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 335\n",
      "After filtering: 74\n"
     ]
    }
   ],
   "source": [
    "scene_obj_nodes = filter_scene_nodes(scene_obj_nodes, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merging: 74\n",
      "After merging: 74\n"
     ]
    }
   ],
   "source": [
    "scene_obj_nodes = merge_scene_nodes(scene_obj_nodes, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1 sources: [('636', 0), ('650', 0), ('630', 0), ('642', 0), ('655', 0), ('658', 6), ('641', 0), ('634', 0), ('649', 0), ('639', 0), ('656', 0), ('632', 0), ('643', 0), ('633', 0), ('629', 1), ('653', 0), ('651', 0), ('645', 0), ('647', 0), ('654', 1), ('648', 0), ('625', 3), ('646', 0), ('659', 0), ('644', 0), ('637', 0), ('635', 1)]\n",
      "office chair swivel chair\n",
      "Node 2 sources: [('682', 3), ('680', 6), ('683', 4), ('709', 3), ('710', 1), ('679', 7), ('681', 0), ('712', 1), ('684', 8), ('711', 2)]\n",
      "swivel\n",
      "Aggregate similarity: tensor([[-inf]], device='cuda:0', dtype=torch.float64)\n",
      "Spatial similarity: tensor([[0.2393]], device='cuda:0', dtype=torch.float64)\n",
      "Visual similarity: tensor([[0.6209]], device='cuda:0')\n",
      "3D Overlap: 0.9999885649630842\n"
     ]
    }
   ],
   "source": [
    "node_id_1 = 217\n",
    "node_id_2 = 114\n",
    "\n",
    "print(f\"Node 1 sources: {scene_obj_nodes[node_id_1]['source_ids']}\")\n",
    "pcd1_img_id, pcd1_obj_id = scene_obj_nodes[node_id_1]['source_ids'][0]\n",
    "print(img_dict[pcd1_img_id]['objs'][pcd1_obj_id]['phrase'])\n",
    "\n",
    "print(f\"Node 2 sources: {scene_obj_nodes[node_id_2]['source_ids']}\")\n",
    "pcd2_img_id, pcd1_obj_id = scene_obj_nodes[node_id_2]['source_ids'][1]\n",
    "print(img_dict[pcd2_img_id]['objs'][pcd1_obj_id]['phrase'])\n",
    "\n",
    "\n",
    "# make a new scene_obj_nodes with only the two nodes\n",
    "node1 = {node_id_1: scene_obj_nodes[node_id_1]}\n",
    "node2 = {node_id_2: scene_obj_nodes[node_id_2]}\n",
    "aggregate_sim, spatial_sim, visual_sim = compute_aggregate_similarity(node1, node2, params)\n",
    "\n",
    "print(f\"Aggregate similarity: {aggregate_sim}\")\n",
    "print(f\"Spatial similarity: {spatial_sim}\")\n",
    "print(f\"Visual similarity: {visual_sim}\")\n",
    "\n",
    "node1_bbox = node1[node_id_1]['bbox']\n",
    "node2_bbox = node2[node_id_2]['bbox']\n",
    "max_overlap = compute_3d_iou(node1_bbox, node2_bbox, use_iou=False)\n",
    "print(f\"3D Overlap: {max_overlap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objs in the scene:  74\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of objs in the scene: \", len(scene_obj_nodes))\n",
    "for node_id, node_data in scene_obj_nodes.items():\n",
    "    o3d.io.write_point_cloud(f'/scratch/kumaradi.gupta/kinect_pcds/{node_id}.pcd', node_data['pcd'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
