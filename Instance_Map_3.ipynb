{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import supervision as sv\n",
    "import open3d as o3d\n",
    "from cuml.cluster import DBSCAN\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import box_convert\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_dir = \"/scratch/kumaradi.gupta/rtab_data/rgb\"\n",
    "depth_dir = \"/scratch/kumaradi.gupta/rtab_data/depth/\"\n",
    "pose_dir = \"/scratch/kumaradi.gupta/rtab_data/pose/\"\n",
    "\n",
    "img_dict_dir = \"/home2/kumaradi.gupta/instance-map/rtab_img_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle file\n",
    "with open(img_dict_dir, 'rb') as file:\n",
    "    img_dict = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "img_dict = {img_name: {img_path: str,\n",
    "                        ram_tags: list_of_str,\n",
    "                        objs: {0: {bbox: [x1, y1, x2, y2],\n",
    "                                    phrase: str,\n",
    "                                    clip_embed: [1, 1024]},\n",
    "                                    dino_embed: [1, 1024]},\n",
    "                                    mask: [h, w],\n",
    "                                    prob: float,\n",
    "                                    aabb: arr}\n",
    "                                1: {...},\n",
    "                        }\n",
    "            img_name: {...},\n",
    "            }\n",
    "'''\n",
    "\n",
    "def get_depth(img_name):\n",
    "    # depth_path = os.path.join(depth_dir, img_name + '.npy')\n",
    "    # depth = np.load(depth_path)\n",
    "\n",
    "    depth_path = os.path.join(depth_dir, img_name + '.png')\n",
    "    depth = cv2.imread(depth_path, cv2.IMREAD_ANYDEPTH)\n",
    "    depth = depth.astype(np.float32) / 1000.0\n",
    "    return depth\n",
    "\n",
    "def get_pose(img_name):\n",
    "    pose_path = os.path.join(pose_dir, img_name + '.txt')\n",
    "\n",
    "    # check if the pose file exists, if it doesn't, return None\n",
    "    if not os.path.exists(pose_path):\n",
    "        return None\n",
    "    \n",
    "    with open(pose_path, 'r') as f:\n",
    "        pose = f.read().split()\n",
    "        pose = np.array(pose).astype(np.float32)\n",
    "    return pose\n",
    "\n",
    "def get_sim_cam_mat_with_fov(h, w, fov):\n",
    "    cam_mat = np.eye(3)\n",
    "    cam_mat[0, 0] = cam_mat[1, 1] = w / (2.0 * np.tan(np.deg2rad(fov / 2)))\n",
    "    cam_mat[0, 2] = w / 2.0\n",
    "    cam_mat[1, 2] = h / 2.0\n",
    "    return cam_mat\n",
    "\n",
    "def get_realsense_cam_mat():\n",
    "    K = np.array([[386.458, 0, 321.111],\n",
    "              [0, 386.458, 241.595],\n",
    "              [0, 0, 1]])\n",
    "    return K\n",
    "\n",
    "def get_kinect_cam_mat(): #TODO: add kinect cam matrix\n",
    "    K = np.array([[386.458, 0, 321.111],\n",
    "              [0, 386.458, 241.595],\n",
    "              [0, 0, 1]])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_point_cloud(obj_data, cam_mat, color=(1, 0, 0), cam_height=0.9):\n",
    "    \"\"\"\n",
    "    Generates a point cloud from a depth image, camera intrinsics, mask, and pose.\n",
    "    Only points within the mask and with valid depth are added to the cloud.\n",
    "    Points are colored using the specified color.\n",
    "    \"\"\"\n",
    "\n",
    "    depth = get_depth(obj_data['img_id'])\n",
    "    pose = get_pose(obj_data['img_id'])\n",
    "    mask = obj_data['mask']\n",
    "\n",
    "    # Reproject the depth to 3D space\n",
    "    rows, cols = np.where(mask)\n",
    "\n",
    "    depth_values = depth[rows, cols]\n",
    "    valid_depth_indices = (depth_values > 0) & (depth_values <= 5)\n",
    "\n",
    "    rows = rows[valid_depth_indices]\n",
    "    cols = cols[valid_depth_indices]\n",
    "    depth_values = depth_values[valid_depth_indices]\n",
    "\n",
    "    points2d = np.vstack([cols, rows, np.ones_like(rows)])\n",
    "\n",
    "    cam_mat_inv = np.linalg.inv(cam_mat)\n",
    "    points3d_cam = cam_mat_inv @ points2d * depth_values\n",
    "\n",
    "    # Parse the pose\n",
    "    pos = np.array(pose[:3], dtype=float).reshape((3, 1))\n",
    "    quat = pose[3:]\n",
    "    rot = R.from_quat(quat).as_matrix()\n",
    "\n",
    "    # # Apply rotation correction, to match the orientation z: backward, y: upward, and x: right\n",
    "    # rot_ro_cam = np.eye(3)\n",
    "    # rot_ro_cam[1, 1] = -1\n",
    "    # rot_ro_cam[2, 2] = -1\n",
    "    # rot = rot @ rot_ro_cam\n",
    "\n",
    "    # # Apply position correction\n",
    "    # pos[1] += cam_height\n",
    "\n",
    "    # Create the pose matrix\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3, :3] = rot\n",
    "    pose_matrix[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    # Transform the points to global frame\n",
    "    points3d_homo = np.vstack([points3d_cam, np.ones((1, points3d_cam.shape[1]))])\n",
    "    points3d_global_homo = pose_matrix @ points3d_homo\n",
    "    points3d_global = points3d_global_homo[:3, :]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points3d_global.T)\n",
    "\n",
    "    # Assign color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.tile(color, (points3d_global.shape[1], 1)))\n",
    "\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_DBSCAN(point_cloud_o3d, eps=0.2, min_samples=20):\n",
    "    # Convert Open3D point cloud to NumPy arrays\n",
    "    points_np = np.asarray(point_cloud_o3d.points)\n",
    "    colors_np = np.asarray(point_cloud_o3d.colors)\n",
    "\n",
    "    # Convert NumPy array to CuPy array for GPU computations\n",
    "    points_gpu = cp.asarray(points_np)\n",
    "\n",
    "    # Create a DBSCAN instance with cuML\n",
    "    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Fit the model to the GPU data\n",
    "    dbscan_model.fit(points_gpu)\n",
    "\n",
    "    # Get the labels for the clusters\n",
    "    labels_gpu = dbscan_model.labels_\n",
    "\n",
    "    # Convert the labels back to a NumPy array\n",
    "    labels = cp.asnumpy(labels_gpu)\n",
    "\n",
    "    # Filter the points and colors that are part of a cluster (label != -1)\n",
    "    clustered_points = points_np[labels != -1]\n",
    "    clustered_colors = colors_np[labels != -1]\n",
    "\n",
    "    # Create a new Open3D point cloud with the clustered points and colors\n",
    "    filtered_point_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    filtered_point_cloud_o3d.points = o3d.utility.Vector3dVector(clustered_points)\n",
    "    filtered_point_cloud_o3d.colors = o3d.utility.Vector3dVector(clustered_colors)\n",
    "\n",
    "    return filtered_point_cloud_o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cosine_similarity(vec1, vec2):\n",
    "    # Ensure the vectors have the same shape\n",
    "    if vec1.shape != vec2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "\n",
    "    # Compute the dot product of the vectors\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # Compute the magnitudes (Euclidean norms) of the vectors\n",
    "    magnitude_vec1 = np.linalg.norm(vec1)\n",
    "    magnitude_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "\n",
    "    # Normalize the similarity value to [0, 1]\n",
    "    normalized_similarity = 0.5 * (similarity + 1)\n",
    "\n",
    "    return normalized_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obj Nodes Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(clip_embed, node_clip_embed):\n",
    "    # Compute the cosine similarity\n",
    "    sim = custom_cosine_similarity([clip_embed], [node_clip_embed])[0][0]\n",
    "    # Normalize between 0 and 1\n",
    "    norm_sim = 0.5 * (1 + sim)\n",
    "    return norm_sim\n",
    "\n",
    "def nnratio(pcd, node_pcd, delta_nn):\n",
    "    # Use KDTree for nearest neighbor search\n",
    "    tree = cKDTree(node_pcd)\n",
    "    distances, _ = tree.query(pcd, distance_upper_bound=delta_nn)\n",
    "    \n",
    "    # Proportion of points within distance threshold\n",
    "    ratio = np.sum(distances != np.inf) / len(pcd)\n",
    "    return ratio\n",
    "\n",
    "def delta_sim(pcd, node_pcd, clip_embed, node_clip_embed, params):\n",
    "    delta_geo = nnratio(pcd, node_pcd, params['delta_nn'])\n",
    "    delta_sem = cosine_sim(clip_embed, node_clip_embed)\n",
    "\n",
    "    delta_sim = delta_geo + delta_sem\n",
    "    \n",
    "    return delta_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nodes(node1, node2):\n",
    "    # Merge source IDs: source_ids: [(img_id, obj_id), ...]\n",
    "    source_ids = node1['source_ids'] + node2['source_ids']\n",
    "    count = len(source_ids)\n",
    "\n",
    "    # Average the embeddings\n",
    "    avg_clip_embed = (np.array(node1['clip_embed']) * len(node1['source_ids']) +\n",
    "                      np.array(node2['clip_embed']) * len(node2['source_ids'])) / count\n",
    "\n",
    "    avg_dino_embed = (np.array(node1['dino_embed']) * len(node1['source_ids']) +\n",
    "                      np.array(node2['dino_embed']) * len(node2['source_ids'])) / count\n",
    "\n",
    "    # Combine point clouds\n",
    "    merged_pcd = node1['pcd']\n",
    "    merged_pcd.points.extend(node2['pcd'].points)\n",
    "\n",
    "    # Concatenate the points contributions from both nodes\n",
    "    points_contri = node1['points_contri'] + node2['points_contri']\n",
    "\n",
    "    return {\n",
    "        'source_ids': source_ids,\n",
    "        'clip_embed': avg_clip_embed,\n",
    "        'dino_embed': avg_dino_embed,\n",
    "        'pcd': merged_pcd,\n",
    "        'points_contri': points_contri\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merge_scene_node_id(similarities, scene_obj_nodes, params):\n",
    "    # Find the node with the minimum similarity value, greedy assignment\n",
    "    max_sim_node_id = max(similarities, key=similarities.get)\n",
    "\n",
    "    # Check if the similarity is below the threshold\n",
    "    if similarities[max_sim_node_id] >= params['sim_thresh']:\n",
    "        return max_sim_node_id\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_scene_nodes(img_dict, params):\n",
    "    # Initialize an empty dictionary to store scene object nodes\n",
    "    scene_obj_nodes = {}\n",
    "\n",
    "    # Retrieve the initial image data using the provided ID\n",
    "    img_data = img_dict[params['init_img_id']]\n",
    "    img_path = img_data['img_path']\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image from BGR to RGB format\n",
    "\n",
    "    # Retrieve objects present in the image\n",
    "    objs = img_data['objs']\n",
    "\n",
    "    for obj_id in objs:\n",
    "        obj_data = objs[obj_id]\n",
    "        \n",
    "        # Create a point cloud for the object\n",
    "        pcd = create_point_cloud(obj_data, params['cam_mat']) #TODO: add color\n",
    "        pcd = pcd.voxel_down_sample(voxel_size=params['voxel_size'])\n",
    "        pcd = fast_DBSCAN(pcd, eps=params['eps'], min_samples=params['min_samples'])\n",
    "\n",
    "        # Store the object data in the scene object nodes dictionary\n",
    "        scene_obj_nodes[obj_id] = {'source_ids': [(params['init_img_id'], obj_id)], \n",
    "                                   'clip_embed': objs[obj_id]['clip_embed'], \n",
    "                                   'dino_embed': objs[obj_id]['dino_embed'], \n",
    "                                   'pcd': pcd, \n",
    "                                   'points_contri': [len(pcd.points)]}  # Count of points in the point cloud\n",
    "\n",
    "    return scene_obj_nodes\n",
    "\n",
    "def compute_obj_similarity(node_id, node_data, pcd, clip_embed, params):\n",
    "    # Calculate and return the similarity between the provided point cloud (pcd) and node's point cloud\n",
    "    return node_id, delta_sim(pcd, node_data['pcd'], clip_embed, node_data['clip_embed'], params)\n",
    "\n",
    "def process_object(obj_data, scene_obj_nodes, params):\n",
    "    # Create a point cloud for the object\n",
    "    pcd = create_point_cloud(obj_data, params['cam_mat']) #TODO: add color\n",
    "    # Down-sample the point cloud\n",
    "    pcd = pcd.voxel_down_sample(voxel_size=params['voxel_size'])\n",
    "    # Apply fast DBSCAN clustering\n",
    "    pcd = fast_DBSCAN(pcd, eps=params['eps'], min_samples=params['min_samples'])\n",
    "    \n",
    "    # Compute similarities between the object and all nodes in the scene\n",
    "    similarities = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_sims = {executor.submit(compute_obj_similarity, node_id, node_data, pcd, obj_data['clip_embed'], params): node_id for node_id, node_data in scene_obj_nodes.items()}\n",
    "        for future in concurrent.futures.as_completed(future_sims):\n",
    "            node_id = future_sims[future]\n",
    "            similarities[node_id] = future.result()[1]\n",
    "        \n",
    "    return similarities, pcd\n",
    "\n",
    "def update_scene_nodes(img_id, img_data, scene_obj_nodes, params):\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for obj_id, obj_data in img_data['objs'].items():\n",
    "            # Start asynchronous processing for each object\n",
    "            future = executor.submit(process_object, obj_data, scene_obj_nodes, params)\n",
    "            futures.append((obj_id, img_id, future))\n",
    "\n",
    "        for obj_id, current_img_id, future in futures:\n",
    "            similarities, obj_pcd = future.result()\n",
    "            \n",
    "            # Determine whether to merge the object with an existing node or create a new one (id or None)\n",
    "            merge_scene_node_id = get_merge_scene_node_id(similarities, scene_obj_nodes, params)\n",
    "\n",
    "            if merge_scene_node_id is not None:\n",
    "                new_node = {\n",
    "                    'source_ids': [(obj_id, current_img_id)],\n",
    "                    'clip_embed': img_data['objs'][obj_id]['clip_embed'],\n",
    "                    'dino_embed': img_data['objs'][obj_id]['dino_embed'],\n",
    "                    'pcd': obj_pcd,\n",
    "                    'points_contri': [len(obj_pcd.points)]}\n",
    "                \n",
    "                scene_node = scene_obj_nodes[merge_scene_node_id]\n",
    "                \n",
    "                scene_obj_nodes[merge_scene_node_id] = merge_nodes(scene_node, new_node)\n",
    "            else:\n",
    "                new_node_id = (max([int(i) for i in scene_obj_nodes.keys()]) + 1)\n",
    "                scene_obj_nodes[new_node_id] = {\n",
    "                    'source_ids': [(obj_id, current_img_id)],\n",
    "                    'clip_embed': img_data['objs'][obj_id]['clip_embed'],\n",
    "                    'dino_embed': img_data['objs'][obj_id]['dino_embed'],\n",
    "                    'pcd': obj_pcd,\n",
    "                    'points_contri': [len(obj_pcd.points)]\n",
    "                }\n",
    "\n",
    "    return scene_obj_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_similarity(node1_id, node1_data, node2_id, node2_data, params):\n",
    "    # Computes the similarity between two nodes.\n",
    "    similarity = delta_sim(node1_data['pcd'], node2_data['pcd'], \n",
    "                           node1_data['clip_embed'], node2_data['clip_embed'], params)\n",
    "    return node1_id, node2_id, similarity\n",
    "\n",
    "def merge_similar_nodes(scene_obj_nodes, params):\n",
    "    # Merges nodes in the scene that have high similarity.\n",
    "    nodes_to_remove = set()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        \n",
    "        # Create a list of node pairs to check\n",
    "        node_ids = list(scene_obj_nodes.keys())\n",
    "        \n",
    "        for i in range(len(node_ids)):\n",
    "            for j in range(i+1, len(node_ids)):\n",
    "                node1_id = node_ids[i]\n",
    "                node2_id = node_ids[j]\n",
    "\n",
    "                # Avoid re-checking nodes that have already been merged\n",
    "                if node1_id in nodes_to_remove or node2_id in nodes_to_remove:\n",
    "                    continue\n",
    "\n",
    "                future = executor.submit(compute_node_similarity, node1_id, scene_obj_nodes[node1_id],\n",
    "                                         node2_id, scene_obj_nodes[node2_id], params)\n",
    "                futures.append(future)\n",
    "        \n",
    "        # Process the computed similarities and merge nodes if needed\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            node1_id, node2_id, similarity = future.result()\n",
    "            \n",
    "            if similarity >= params['sim_thresh']:\n",
    "                merged_node = merge_nodes(scene_obj_nodes[node1_id], scene_obj_nodes[node2_id])\n",
    "                scene_obj_nodes[node1_id] = merged_node\n",
    "                \n",
    "                # Mark the second node for removal after merging\n",
    "                nodes_to_remove.add(node2_id)\n",
    "\n",
    "    # Remove nodes that were merged into other nodes\n",
    "    for node_id in nodes_to_remove:\n",
    "        del scene_obj_nodes[node_id]\n",
    "\n",
    "    return scene_obj_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'init_img_id': '1',\n",
    "          'sim_thresh': 1.1,\n",
    "          'voxel_size': 0.02,\n",
    "          'eps': 0.2,\n",
    "          'min_samples': 20,\n",
    "          'delta_nn': 0.02,\n",
    "          'cam_mat': get_kinect_cam_mat()}\n",
    "\n",
    "scene_obj_nodes = init_scene_nodes(img_dict, params)\n",
    "scene_obj_nodes = merge_similar_nodes(scene_obj_nodes, params)\n",
    "\n",
    "counter = 0\n",
    "for img_id, img_data in tqdm(img_dict.items()):\n",
    "    if len(img_data['objs']) == 0 or img_id == params['init_img_id']:\n",
    "        continue\n",
    "\n",
    "    scene_obj_nodes = update_scene_nodes(img_id, img_data, scene_obj_nodes, params)\n",
    "    \n",
    "    counter += 1\n",
    "    if counter % 100 == 0:\n",
    "        scene_obj_nodes = merge_similar_nodes(scene_obj_nodes, params)\n",
    "\n",
    "\n",
    "print(\"Number of nodes in the scene: \", len(scene_obj_nodes))\n",
    "for node_id, node_data in scene_obj_nodes.items():\n",
    "    o3d.io.write_point_cloud(f'./node_pcds/{node_id}.pcd', node_data['pcd'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
