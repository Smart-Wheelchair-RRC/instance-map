{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import supervision as sv\n",
    "import open3d as o3d\n",
    "from cuml.cluster import DBSCAN\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install pytorch and add imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add correct paths\n",
    "\n",
    "imgs_dir = \"/scratch/laksh.nanwani/instance_slam/JmbYfDe2QKZ/rgb\"\n",
    "depth_dir = \"/scratch/laksh.nanwani/instance_slam/JmbYfDe2QKZ/depth/\"\n",
    "pose_dir = \"/scratch/laksh.nanwani/instance_slam/JmbYfDe2QKZ/pose/\"\n",
    "img_dict_dir = \"/home2/kumaradi.gupta/instance-map/img_dict.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_tags = ['chair', 'clock', 'closet', 'stool', 'table', 'couch', 'fan', 'door']\n",
    "\n",
    "colors = [\n",
    "    (255, 105, 97),  # Pastel Red\n",
    "    (255, 179, 71),  # Pastel Orange\n",
    "    (253, 253, 150),  # Pastel Yellow\n",
    "    (119, 221, 119),  # Pastel Green\n",
    "    (203, 153, 201),  # Pastel Purple\n",
    "    (174, 198, 207),  # Pastel Blue\n",
    "    (255, 209, 220),  # Pastel Pink\n",
    "    (0, 255, 0)       # Green\n",
    "]\n",
    "\n",
    "def normalize_color(color_255):\n",
    "    return tuple(val/255.0 for val in color_255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle file\n",
    "with open(img_dict_dir, 'rb') as file:\n",
    "    img_dict = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D AABB Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "img_dict = {img_name: {img_path: str,\n",
    "                        ram_tags: list_of_str,\n",
    "                        objs: {0: {bbox: [x1, y1, x2, y2],\n",
    "                                    phrase: str,\n",
    "                                    clip_embed: [1, 1024]},\n",
    "                                    dino_embed: [1, 1024]},\n",
    "                                    mask: [h, w],\n",
    "                                    prob: float,\n",
    "                                    aabb: arr}\n",
    "                                1: {...},\n",
    "                        }\n",
    "            img_name: {...},\n",
    "            }\n",
    "'''\n",
    "\n",
    "# go through the img_name and their data\n",
    "# open the depth associated with an image from the depth folder (.npy file)\n",
    "\n",
    "def get_depth(img_name):\n",
    "    depth_path = os.path.join(depth_dir, img_name + '.npy')\n",
    "    depth = np.load(depth_path)\n",
    "    return depth\n",
    "\n",
    "def get_pose(img_name):\n",
    "    pose_path = os.path.join(pose_dir, img_name + '.txt')\n",
    "    with open(pose_path, 'r') as f:\n",
    "        pose = f.read().split()\n",
    "        pose = np.array(pose).astype(np.float32)\n",
    "    return pose\n",
    "\n",
    "def get_sim_cam_mat_with_fov(h, w, fov):\n",
    "    cam_mat = np.eye(3)\n",
    "    cam_mat[0, 0] = cam_mat[1, 1] = w / (2.0 * np.tan(np.deg2rad(fov / 2)))\n",
    "    cam_mat[0, 2] = w / 2.0\n",
    "    cam_mat[1, 2] = h / 2.0\n",
    "    return cam_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_point_cloud(depth, mask, cam_mat, pose, color=(1, 0, 0), camera_height=0.9):\n",
    "    \"\"\"\n",
    "    Generates a point cloud from a depth image, camera intrinsics, mask, and pose.\n",
    "    Only points within the mask and with valid depth are added to the cloud.\n",
    "    Points are colored using the specified color.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reproject the depth to 3D space\n",
    "    rows, cols = np.where(mask)\n",
    "    points2d = np.vstack([cols, rows, np.ones_like(rows)])\n",
    "    depth_values = depth[rows, cols]\n",
    "    cam_mat_inv = np.linalg.inv(cam_mat)\n",
    "    points3d_cam = cam_mat_inv @ points2d * depth_values\n",
    "\n",
    "    # Parse the pose\n",
    "    pos = np.array(pose[:3], dtype=float).reshape((3, 1))\n",
    "    quat = pose[3:]\n",
    "    rot = R.from_quat(quat).as_matrix()\n",
    "\n",
    "    # Apply rotation correction, to match the orientation z: backward, y: upward, and x: right\n",
    "    rot_ro_cam = np.eye(3)\n",
    "    rot_ro_cam[1, 1] = -1\n",
    "    rot_ro_cam[2, 2] = -1\n",
    "    rot = rot @ rot_ro_cam\n",
    "\n",
    "    # Apply position correction\n",
    "    pos[1] += camera_height\n",
    "\n",
    "    # Create the pose matrix\n",
    "    pose_matrix = np.eye(4)\n",
    "    pose_matrix[:3, :3] = rot\n",
    "    pose_matrix[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    # Transform the points to global frame\n",
    "    points3d_homo = np.vstack([points3d_cam, np.ones((1, points3d_cam.shape[1]))])\n",
    "    points3d_global_homo = pose_matrix @ points3d_homo\n",
    "    points3d_global = points3d_global_homo[:3, :]\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points3d_global.T)\n",
    "\n",
    "    # Assign color to the point cloud\n",
    "    pcd.colors = o3d.utility.Vector3dVector(np.tile(color, (points3d_global.shape[1], 1)))\n",
    "\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_bounds(obj_pc):\n",
    "    aabb = obj_pc.get_axis_aligned_bounding_box()\n",
    "    min_bound = aabb.min_bound\n",
    "    max_bound = aabb.max_bound\n",
    "    obj_bounds = np.vstack((min_bound, max_bound))\n",
    "    return obj_bounds\n",
    "\n",
    "def iou_3d(box1, box2):\n",
    "    min_intersection = np.maximum(box1[0], box2[0])\n",
    "    max_intersection = np.minimum(box1[1], box2[1])\n",
    "\n",
    "    # Compute the intersection volume\n",
    "    intersection_dims = np.maximum(max_intersection - min_intersection, 0)\n",
    "    intersection_volume = np.prod(intersection_dims)\n",
    "\n",
    "    # Compute the volumes of the individual bounding boxes\n",
    "    volume_box1 = np.prod(box1[1] - box1[0])\n",
    "    volume_box2 = np.prod(box2[1] - box2[0])\n",
    "\n",
    "    # Compute the union volume\n",
    "    union_volume = volume_box1 + volume_box2 - intersection_volume\n",
    "\n",
    "    # Compute the IoU\n",
    "    iou = intersection_volume / union_volume if union_volume != 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_noise_dbscan(point_cloud, eps=0.1, min_points=15):\n",
    "    # Apply DBSCAN clustering to the point cloud\n",
    "    # with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Debug) as cm:\n",
    "    #     labels = np.array(point_cloud.cluster_dbscan(eps=eps, min_points=min_points, print_progress=False))\n",
    "\n",
    "    labels = np.array(point_cloud.cluster_dbscan(eps=eps, min_points=min_points, print_progress=False))\n",
    "\n",
    "    # Find the largest cluster\n",
    "    max_label = max(labels, key=list(labels).count)\n",
    "\n",
    "    # Filter the point cloud to include only the largest cluster\n",
    "    filtered_points = np.asarray(point_cloud.points)[labels == max_label]\n",
    "    filtered_colors = np.asarray(point_cloud.colors)[labels == max_label]\n",
    "\n",
    "    # Create a new point cloud with only the largest cluster\n",
    "    filtered_pcd = o3d.geometry.PointCloud()\n",
    "    filtered_pcd.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "    filtered_pcd.colors = o3d.utility.Vector3dVector(filtered_colors)\n",
    "\n",
    "    return filtered_pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_DBSCAN(point_cloud_o3d, eps=0.01, min_samples=20):\n",
    "    # Convert Open3D point cloud to NumPy array\n",
    "    points_np = np.asarray(point_cloud_o3d.points)\n",
    "\n",
    "    # Convert NumPy array to CuPy array for GPU computations\n",
    "    points_gpu = cp.asarray(points_np)\n",
    "\n",
    "    # Create a DBSCAN instance with cuML\n",
    "    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "\n",
    "    # Fit the model to the GPU data\n",
    "    dbscan_model.fit(points_gpu)\n",
    "\n",
    "    # Get the labels for the clusters\n",
    "    labels_gpu = dbscan_model.labels_\n",
    "\n",
    "    # Convert the labels back to a NumPy array\n",
    "    labels = cp.asnumpy(labels_gpu)\n",
    "\n",
    "    # Filter the points that are part of a cluster (label != -1)\n",
    "    clustered_points = points_np[labels != -1]\n",
    "\n",
    "    # Create a new Open3D point cloud with the clustered points\n",
    "    filtered_point_cloud_o3d = o3d.geometry.PointCloud()\n",
    "    filtered_point_cloud_o3d.points = o3d.utility.Vector3dVector(clustered_points)\n",
    "\n",
    "    return filtered_point_cloud_o3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_id, img_data in tqdm(img_dict.items()):\n",
    "\n",
    "  if len(img_data['objs']) == 0:\n",
    "    continue\n",
    "\n",
    "  # img_path = img_data['img_path']\n",
    "  # img = cv2.imread(img_path)\n",
    "\n",
    "  depth = get_depth(img_id)\n",
    "  pose = get_pose(img_id)\n",
    "  cam_mat = get_sim_cam_mat_with_fov(depth.shape[0], depth.shape[1], fov=90)\n",
    "\n",
    "  for obj_id, obj_data in img_data['objs'].items():\n",
    "    mask = obj_data['mask']\n",
    "    phrase = obj_data['phrase']\n",
    "    phrase = phrase.split()[0]\n",
    "\n",
    "    pcd = create_point_cloud(depth, mask, cam_mat, pose)\n",
    "\n",
    "    obj_data['aabb'] = get_obj_bounds(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated img_dict with aabb\n",
    "\n",
    "with open('/content/drive/MyDrive/habitat_output/img_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(img_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to merge two clusters\n",
    "def merge_data(cluster1, cluster2):\n",
    "    # Compute the union of the bounding boxes\n",
    "    obj_bounds1 = cluster1['aabb']\n",
    "    obj_bounds2 = cluster2['aabb']\n",
    "\n",
    "    min_bound = np.minimum(obj_bounds1[0], obj_bounds2[0])\n",
    "    max_bound = np.maximum(obj_bounds1[1], obj_bounds2[1])\n",
    "\n",
    "    # Merge source IDs: source_ids: [(img_id, obj_id), ...]\n",
    "    source_ids = cluster1['source_ids'] + cluster2['source_ids']\n",
    "    count = len(source_ids)\n",
    "\n",
    "    # Average the embeddings\n",
    "    avg_dino_embed = (np.array(cluster1['dino_embed']) * len(cluster1['source_ids']) +\n",
    "                      np.array(cluster2['dino_embed']) * len(cluster2['source_ids'])) / count\n",
    "    avg_clip_embed = (np.array(cluster1['clip_embed']) * len(cluster1['source_ids']) +\n",
    "                      np.array(cluster2['clip_embed']) * len(cluster2['source_ids'])) / count\n",
    "    return {\n",
    "        'aabb': np.vstack((min_bound, max_bound)),\n",
    "        'source_ids': source_ids,\n",
    "        'dino_embed': avg_dino_embed,\n",
    "        'clip_embed': avg_clip_embed\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old clustering algo: works but is wrong\n",
    "\n",
    "def cluster_objects(img_dict, iou_threshold=0.5):\n",
    "    # A dictionary to store object-level data\n",
    "    obj_dict = {}\n",
    "    cluster_counter = 0\n",
    "\n",
    "    # Cluster heirarchy 1\n",
    "    # Iterate through the images and objects\n",
    "    for img_id, img_data in tqdm(img_dict.items()):\n",
    "\n",
    "        if len(img_data['objs']) == 0:\n",
    "          continue\n",
    "\n",
    "        for obj_id, obj_data in img_data['objs'].items():\n",
    "          \n",
    "            # Skip the object if aabb does not exist, in case of partial data only\n",
    "            if 'aabb' not in obj_data:\n",
    "                continue\n",
    "\n",
    "            phrase = obj_data['phrase']\n",
    "            phrase = phrase.split()[0]\n",
    "\n",
    "            if phrase == 'armchair': #for testing only\n",
    "              phrase = 'chair'\n",
    "\n",
    "            # Initialize the phrase entry if not present\n",
    "            if phrase not in obj_dict:\n",
    "                obj_dict[phrase] = {}\n",
    "\n",
    "            # Iterate through existing clusters for this phrase\n",
    "            found_cluster = False\n",
    "            for cluster_id, cluster_data in list(obj_dict[phrase].items()):\n",
    "                # Get the IoU for the current box and the cluster\n",
    "                iou = iou_3d(obj_data['aabb'], cluster_data['aabb'])\n",
    "\n",
    "                if iou > iou_threshold:\n",
    "                    # If IoU is above the threshold, update the cluster\n",
    "                    found_cluster = True\n",
    "                    obj_dict[phrase][cluster_id] = merge_data(cluster_data, {\n",
    "                        'aabb': obj_data['aabb'],\n",
    "                        'source_ids': [(img_id, obj_id)],\n",
    "                        'dino_embed': obj_data['dino_embed'],\n",
    "                        'clip_embed': obj_data['clip_embed']\n",
    "                    })\n",
    "\n",
    "                    # Check for possible merges with other clusters\n",
    "                    for other_cluster_id, other_cluster_data in list(obj_dict[phrase].items()):\n",
    "                        if other_cluster_id != cluster_id:\n",
    "                            iou_merge = iou_3d(obj_dict[phrase][cluster_id]['aabb'],\n",
    "                                               other_cluster_data['aabb'])\n",
    "\n",
    "                            if iou_merge > iou_threshold:\n",
    "                                merged_cluster = merge_data(obj_dict[phrase][cluster_id],\n",
    "                                                            other_cluster_data)\n",
    "                                obj_dict[phrase][cluster_id] = merged_cluster\n",
    "                                del obj_dict[phrase][other_cluster_id]\n",
    "\n",
    "                    break\n",
    "\n",
    "            if not found_cluster:\n",
    "                # If no cluster is found, add a new cluster\n",
    "                cluster_id = len(obj_dict[phrase]) # Doesn't work because of arbitrary merging\n",
    "                obj_dict[phrase][cluster_id] = {\n",
    "                    'aabb': obj_data['aabb'],\n",
    "                    'source_ids': [(img_id, obj_id)],\n",
    "                    'dino_embed': obj_data['dino_embed'],\n",
    "                    'clip_embed': obj_data['clip_embed']\n",
    "                }\n",
    "                cluster_counter += 1\n",
    "\n",
    "    # Not good: Final merge check among remaining clusters\n",
    "    # for phrase, clusters in obj_dict.items():\n",
    "    #     cluster_ids = list(clusters.keys())\n",
    "    #     i = 0\n",
    "    #     while i < len(cluster_ids):\n",
    "    #         cluster_id = cluster_ids[i]\n",
    "    #         cluster_data = clusters[cluster_id]\n",
    "    #         merged = False\n",
    "    #         for other_cluster_id, other_cluster_data in list(clusters.items()):\n",
    "    #             if other_cluster_id != cluster_id:\n",
    "    #                 iou_merge = iou_3d(cluster_data['aabb'], other_cluster_data['aabb'])\n",
    "    #                 if iou_merge > iou_threshold:\n",
    "    #                     merged_cluster = merge_data(cluster_data, other_cluster_data)\n",
    "    #                     clusters[cluster_id] = merged_cluster\n",
    "    #                     del clusters[other_cluster_id]\n",
    "    #                     cluster_ids.remove(other_cluster_id)\n",
    "    #                     merged = True\n",
    "    #                     break\n",
    "    #         if not merged:\n",
    "    #             i += 1\n",
    "\n",
    "    return obj_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New clustering algo: correct but the results are not great\n",
    "\n",
    "def custom_distance(obj1, obj2):\n",
    "\n",
    "    iou_weight = 0.7\n",
    "    dino_weight = 0.3\n",
    "\n",
    "    iou = iou_3d(obj1['aabb'], obj2['aabb'])\n",
    "\n",
    "    dino_similarity = cosine_similarity(obj1['dino_embed'].reshape(1, -1),\n",
    "                                        obj2['dino_embed'].reshape(1, -1))[0][0]\n",
    "\n",
    "    distance = ((1 - iou) * iou_weight) + ((1 - dino_similarity) * dino_weight)\n",
    "\n",
    "    if distance < 0: # Happening in cases where iou and dino sim = 1\n",
    "      print(f\"Obj1: {obj1['source_ids']}, Obj2: {obj2['source_ids']}\")\n",
    "      distance = 0\n",
    "\n",
    "    return distance\n",
    "\n",
    "def cluster_objects(img_dict, dist_threshold=0.7):\n",
    "    # Group objects by phrase\n",
    "    phrase_objects = defaultdict(list)\n",
    "    for img_id, img_data in img_dict.items():\n",
    "        for obj_id, obj_data in img_data['objs'].items():\n",
    "\n",
    "            obj_data_with_source = obj_data.copy()\n",
    "            obj_data_with_source['source_ids'] = [(img_id, obj_id)]  # Add source information\n",
    "\n",
    "            phrase = obj_data_with_source['phrase'].split()[0]\n",
    "            phrase_objects[phrase].append(obj_data_with_source)\n",
    "\n",
    "    # Dictionary to store the final merged clusters\n",
    "    merged_clusters = {}\n",
    "\n",
    "    # Perform clustering for each phrase\n",
    "    for phrase, objects_list in tqdm(phrase_objects.items()):\n",
    "\n",
    "        # Compute pairwise distances and convert to condensed form\n",
    "        n = len(objects_list)\n",
    "        distances = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):  # Only calculate upper triangular part\n",
    "                distances[i, j] = custom_distance(objects_list[i], objects_list[j])\n",
    "                distances[j, i] = distances[i, j]  # Symmetric matrix\n",
    "        distances = squareform(distances)\n",
    "\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(distances, method='single')\n",
    "\n",
    "        # Cut the dendrogram to form final clusters\n",
    "        cluster_labels = fcluster(Z, t=dist_threshold, criterion='distance')\n",
    "\n",
    "        # Merge clusters for this phrase\n",
    "        phrase_clusters = {}\n",
    "        for idx, label in enumerate(cluster_labels):\n",
    "            if label not in phrase_clusters:\n",
    "                phrase_clusters[label] = objects_list[idx]\n",
    "            else:\n",
    "                # Merge data using your existing merge_data function\n",
    "                phrase_clusters[label] = merge_data(phrase_clusters[label], objects_list[idx])\n",
    "\n",
    "        # Store the merged clusters for this phrase\n",
    "        merged_clusters[phrase] = phrase_clusters\n",
    "\n",
    "    return merged_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_dict = cluster_objects(img_dict, dist_threshold=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(obj_dict['chair']), len(obj_dict['table']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_obj_point_clouds(obj_dict, img_dict, voxel_size=0.05, color=(1, 0, 0)):\n",
    "    # Ensure the outputs directory exists\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.mkdir('outputs')\n",
    "\n",
    "    # Iterate through the phrases and clusters\n",
    "    for phrase, clusters in tqdm(obj_dict.items()):\n",
    "        # Create a directory for this phrase if it doesn't exist\n",
    "        phrase_dir = os.path.join('outputs', phrase)\n",
    "        if not os.path.exists(phrase_dir):\n",
    "            os.mkdir(phrase_dir)\n",
    "\n",
    "        color = colors[req_tags.index(phrase)]\n",
    "        color = normalize_color(color)\n",
    "\n",
    "        # Iterate through the clusters\n",
    "        for cluster_id, cluster_data in tqdm(clusters.items()):\n",
    "            # Initialize an empty point cloud to accumulate points from all source IDs\n",
    "            final_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "            # Iterate through the source IDs for this cluster\n",
    "            for img_id, obj_id in cluster_data['source_ids']:\n",
    "                # Get the required information\n",
    "                depth = get_depth(img_id)\n",
    "                pose = get_pose(img_id)\n",
    "                cam_mat = get_sim_cam_mat_with_fov(depth.shape[0], depth.shape[1], fov=90)\n",
    "                mask = img_dict[img_id]['objs'][obj_id]['mask']\n",
    "\n",
    "                # Create the point cloud for this source ID\n",
    "                pcd = create_point_cloud(depth, mask, cam_mat, pose, color=color)\n",
    "                # Combine with the final point cloud\n",
    "                final_pcd += pcd\n",
    "\n",
    "            # Downsample and filter the final point cloud\n",
    "            final_pcd = final_pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "            final_pcd = filter_noise_dbscan(final_pcd, eps=0.1, min_points=20)\n",
    "\n",
    "\n",
    "            # Save the final point cloud for this cluster\n",
    "            file_path = os.path.join(phrase_dir, f'{cluster_id}.ply')\n",
    "            o3d.io.write_point_cloud(file_path, final_pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd '/content/drive/MyDrive/habitat_output'\n",
    "\n",
    "create_obj_point_clouds(obj_dict, img_dict, voxel_size=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the embeddings of all images, first object instance\n",
    "\n",
    "embedding_type = 'dino_embed'\n",
    "# Sort the img_ids\n",
    "sorted_img_ids = sorted(img_dict.keys(), key=lambda x: [int(i) for i in x.split('_')])\n",
    "\n",
    "# Extract the embeddings in the sorted order\n",
    "embeddings = [img_dict[img_id]['objs'][0][embedding_type] for img_id in sorted_img_ids]\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Visualize the similarity matrix as a heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Clip Embedding Similarity')\n",
    "\n",
    "# Add the similarity scores as text labels\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(len(similarity_matrix)):\n",
    "        plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', ha='center', va='center', color='white' if similarity_matrix[i, j] < 0.5 else 'black')\n",
    "\n",
    "plt.xticks(range(len(img_dict)), sorted_img_ids, rotation=90)\n",
    "plt.yticks(range(len(img_dict)), sorted_img_ids)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to visualize separate masks for a single image\n",
    "\n",
    "img_id = 'JmbYfDe2QKZ_1'\n",
    "\n",
    "# Load the image\n",
    "img_path = img_dict[img_id]['img_path']\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "\n",
    "# Get the object data for the image\n",
    "objs = img_dict[img_id]['objs']\n",
    "\n",
    "# Loop through each object in the image\n",
    "for obj_index, obj_data in objs.items():\n",
    "    # Get the mask for the object\n",
    "    mask = obj_data['mask']\n",
    "\n",
    "    # If the mask isn't a numpy array, convert it to one\n",
    "    if not isinstance(mask, np.ndarray):\n",
    "        mask = mask.numpy()\n",
    "\n",
    "    # Apply the mask to the image. This assumes your mask is binary (0s and 1s).\n",
    "    # If it isn't, you may need to adjust this step accordingly.\n",
    "    masked_image = img * np.expand_dims(mask, axis=2)\n",
    "\n",
    "    # Display the masked image\n",
    "    plt.figure()\n",
    "    plt.imshow(masked_image)\n",
    "    plt.title(f\"Object {obj_index}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to draw the segmentation mask on the image\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "img_id = 'JmbYfDe2QKZ_123'\n",
    "\n",
    "# Load the image\n",
    "img_path = img_dict[img_id]['img_path']\n",
    "img = Image.open(img_path)  # Load with PIL to avoid needing to convert color spaces\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "img_tensor = transform(img)\n",
    "img_tensor = (img_tensor * 255).byte()\n",
    "\n",
    "\n",
    "masks = []\n",
    "mask_colors = []\n",
    "\n",
    "objs = img_dict[img_id]['objs']\n",
    "\n",
    "for obj_index, obj_data in objs.items():\n",
    "    mask = obj_data['mask']\n",
    "\n",
    "    phrase = obj_data['phrase']\n",
    "    phrase = phrase.split()[0] #Use only the first phrase\n",
    "\n",
    "    # If the mask isn't a numpy array, convert it to one\n",
    "    if not isinstance(mask, np.ndarray):\n",
    "        mask = mask.numpy()\n",
    "\n",
    "    # Add the mask to the list of masks\n",
    "    masks.append(mask)\n",
    "    #Cheeck the color for the mask and add it to the list\n",
    "    color = colors[req_tags.index(phrase)]\n",
    "    mask_colors.append(color)\n",
    "\n",
    "# Convert the masks to a boolean tensor\n",
    "masks_tensor = torch.tensor(masks, dtype=torch.bool)\n",
    "\n",
    "# Draw all masks on the image\n",
    "result = draw_segmentation_masks(img_tensor, masks_tensor, colors=mask_colors)\n",
    "\n",
    "# Convert the result tensor back to a PIL image, transpose the dimensions back to H,W,C for PIL\n",
    "result_img = Image.fromarray(result.permute(1, 2, 0).byte().cpu().numpy())\n",
    "\n",
    "# Display the image with all masks\n",
    "plt.imshow(result_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to check if there are multiple phrases for a single object\n",
    "\n",
    "for img_id, img_data in tqdm(img_dict.items()):\n",
    "    objs_data = img_data['objs']\n",
    "    for obj_id, obj_data in objs_data.items():\n",
    "        phrase = obj_data['phrase']\n",
    "        words = phrase.split()  # Splits the phrase into words using whitespace as the separator\n",
    "        if len(words) > 1:  # Checks if there are more than one word\n",
    "            print(f'{img_id}_{obj_id}: {phrase}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to print the similarity of dino embeds for all objects of an image\n",
    "\n",
    "img_id = 'JmbYfDe2QKZ_123'\n",
    "objs_data = img_dict[img_id]['objs']\n",
    "\n",
    "dino_embeds = [obj_data['dino_embed'] for obj_id, obj_data in objs_data.items()]\n",
    "phrases = [obj_data['phrase'] for obj_id, obj_data in objs_data.items()]\n",
    "\n",
    "# Reshaping the embeds to fit the input shape for cosine_similarity\n",
    "dino_embeds_matrix = np.array(dino_embeds).reshape(-1, 768)\n",
    "\n",
    "# Calculating the cosine similarity\n",
    "similarity_matrix = cosine_similarity(dino_embeds_matrix)\n",
    "\n",
    "# print(\"Cosine Similarities:\")\n",
    "# print(similarities)\n",
    "\n",
    "# Visualize the similarity matrix as a heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Dino Embedding Similarity')\n",
    "\n",
    "# Add the similarity scores as text labels\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(len(similarity_matrix)):\n",
    "        plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', ha='center', va='center', color='white' if similarity_matrix[i, j] < 0.5 else 'black')\n",
    "\n",
    "plt.xticks(range(len(phrases)), phrases, rotation=90)\n",
    "plt.yticks(range(len(phrases)), phrases)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 3D bounding boxes without DBSCAN\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "img_ids = ['JmbYfDe2QKZ_123']\n",
    "\n",
    "# Normalize the colors to (0-1) range\n",
    "colors_normalized = [(r/255, g/255, b/255) for r, g, b in colors]\n",
    "\n",
    "# Create a mapping of phrases to colors\n",
    "color_mapping = {phrase: color for phrase, color in zip(req_tags, colors_normalized)}\n",
    "default_color = 'gray' # Default color if the phrase is not in the mapping\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Finding the global min and max bounds across all bounding boxes\n",
    "global_min_bound = np.array([float('inf')] * 3)\n",
    "global_max_bound = np.array([float('-inf')] * 3)\n",
    "\n",
    "for img_id in img_ids:\n",
    "  obj_data = img_dict[img_id]['objs']\n",
    "\n",
    "  for obj_id, data in tqdm(obj_data.items()):\n",
    "\n",
    "      bounds = data['aabb']\n",
    "\n",
    "      phrase = data['phrase']\n",
    "      phrase = phrase.split()[0]\n",
    "\n",
    "      # if phrase != 'chair':\n",
    "      #     continue\n",
    "\n",
    "      color = color_mapping.get(phrase, default_color)\n",
    "\n",
    "      min_bound, max_bound = bounds\n",
    "      min_bound = [min_bound[0], min_bound[2], min_bound[1]]\n",
    "      max_bound = [max_bound[0], max_bound[2], max_bound[1]]\n",
    "\n",
    "      global_min_bound = np.minimum(global_min_bound, min_bound)\n",
    "      global_max_bound = np.maximum(global_max_bound, max_bound)\n",
    "\n",
    "      # Define vertices of the bounding box\n",
    "      vertices = [\n",
    "          [min_bound[0], min_bound[1], min_bound[2]],\n",
    "          [max_bound[0], min_bound[1], min_bound[2]],\n",
    "          [max_bound[0], max_bound[1], min_bound[2]],\n",
    "          [min_bound[0], max_bound[1], min_bound[2]],\n",
    "          [min_bound[0], min_bound[1], max_bound[2]],\n",
    "          [max_bound[0], min_bound[1], max_bound[2]],\n",
    "          [max_bound[0], max_bound[1], max_bound[2]],\n",
    "          [min_bound[0], max_bound[1], max_bound[2]]\n",
    "      ]\n",
    "\n",
    "      # Define faces using vertices\n",
    "      faces = [\n",
    "          [vertices[0], vertices[1], vertices[2], vertices[3]],\n",
    "          [vertices[4], vertices[5], vertices[6], vertices[7]],\n",
    "          [vertices[0], vertices[1], vertices[5], vertices[4]],\n",
    "          [vertices[2], vertices[3], vertices[7], vertices[6]],\n",
    "          [vertices[1], vertices[2], vertices[6], vertices[5]],\n",
    "          [vertices[4], vertices[7], vertices[3], vertices[0]]\n",
    "      ]\n",
    "\n",
    "      # Plot the faces\n",
    "      for face in faces:\n",
    "          poly3d = [[vertice[0], vertice[1], vertice[2]] for vertice in face]\n",
    "          ax.add_collection3d(Poly3DCollection([poly3d], facecolors=color, linewidths=1, edgecolors='k', alpha=0.25))\n",
    "\n",
    "\n",
    "# Setting the axes limits based on the global bounds\n",
    "ax.set_xlim([global_min_bound[0], global_max_bound[0]])\n",
    "ax.set_ylim([global_min_bound[1], global_max_bound[1]])\n",
    "ax.set_zlim([global_min_bound[2], global_max_bound[2]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 3D bounding boxes with DBSCAN\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "img_ids = ['JmbYfDe2QKZ_123']\n",
    "\n",
    "depth = get_depth(img_id)\n",
    "pose = get_pose(img_id)\n",
    "cam_mat = get_sim_cam_mat_with_fov(depth.shape[0], depth.shape[1], fov=90)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Normalize the colors to (0-1) range\n",
    "colors_normalized = [(r/255, g/255, b/255) for r, g, b in colors]\n",
    "\n",
    "# Create a mapping of phrases to colors\n",
    "color_mapping = {phrase: color for phrase, color in zip(req_tags, colors_normalized)}\n",
    "default_color = 'gray' # Default color if the phrase is not in the mapping\n",
    "\n",
    "# Finding the global min and max bounds across all bounding boxes\n",
    "global_min_bound = np.array([float('inf')] * 3)\n",
    "global_max_bound = np.array([float('-inf')] * 3)\n",
    "\n",
    "for img_id in img_ids:\n",
    "  obj_data = img_dict[img_id]['objs']\n",
    "  for obj_id, data in tqdm(obj_data.items()):\n",
    "\n",
    "      #Reconstruct pointcloud\n",
    "      mask = data['mask']\n",
    "      phrase = data['phrase']\n",
    "      phrase = phrase.split()[0]\n",
    "\n",
    "      color = color_mapping.get(phrase, default_color)\n",
    "\n",
    "      pcd = create_point_cloud(depth, mask, cam_mat, pose)\n",
    "\n",
    "      filtered_point_cloud = fast_DBSCAN(pcd)\n",
    "      \n",
    "      pcd = pcd.voxel_down_sample(voxel_size=0.005)\n",
    "      pcd = filter_noise_dbscan(pcd, eps=0.1, min_points=15)\n",
    "\n",
    "      bounds = get_obj_bounds(pcd)\n",
    "      # bounds = data['aabb']\n",
    "      min_bound, max_bound = bounds\n",
    "\n",
    "      min_bound = [min_bound[0], min_bound[2], min_bound[1]]\n",
    "      max_bound = [max_bound[0], max_bound[2], max_bound[1]]\n",
    "\n",
    "      global_min_bound = np.minimum(global_min_bound, min_bound)\n",
    "      global_max_bound = np.maximum(global_max_bound, max_bound)\n",
    "\n",
    "      # Define vertices of the bounding box\n",
    "      vertices = [\n",
    "          [min_bound[0], min_bound[1], min_bound[2]],\n",
    "          [max_bound[0], min_bound[1], min_bound[2]],\n",
    "          [max_bound[0], max_bound[1], min_bound[2]],\n",
    "          [min_bound[0], max_bound[1], min_bound[2]],\n",
    "          [min_bound[0], min_bound[1], max_bound[2]],\n",
    "          [max_bound[0], min_bound[1], max_bound[2]],\n",
    "          [max_bound[0], max_bound[1], max_bound[2]],\n",
    "          [min_bound[0], max_bound[1], max_bound[2]]\n",
    "      ]\n",
    "\n",
    "      # Define faces using vertices\n",
    "      faces = [\n",
    "          [vertices[0], vertices[1], vertices[2], vertices[3]],\n",
    "          [vertices[4], vertices[5], vertices[6], vertices[7]],\n",
    "          [vertices[0], vertices[1], vertices[5], vertices[4]],\n",
    "          [vertices[2], vertices[3], vertices[7], vertices[6]],\n",
    "          [vertices[1], vertices[2], vertices[6], vertices[5]],\n",
    "          [vertices[4], vertices[7], vertices[3], vertices[0]]\n",
    "      ]\n",
    "\n",
    "      # Plot the faces\n",
    "      for face in faces:\n",
    "          poly3d = [[vertice[0], vertice[1], vertice[2]] for vertice in face]\n",
    "          ax.add_collection3d(Poly3DCollection([poly3d], facecolors=color, linewidths=1, edgecolors='k', alpha=0.25))\n",
    "\n",
    "\n",
    "# Setting the axes limits based on the global bounds\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "ax.set_xlim([global_min_bound[0], global_max_bound[0]])\n",
    "ax.set_ylim([global_min_bound[1], global_max_bound[1]])\n",
    "ax.set_zlim([global_min_bound[2], global_max_bound[2]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = 'JmbYfDe2QKZ_123'\n",
    "depth_data = get_depth(img_id)\n",
    "\n",
    "# Visualize the depth data as a heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(depth_data, cmap='viridis')  # You can choose different colormaps like 'gray', 'plasma', etc.\n",
    "plt.colorbar(label='Depth Value')\n",
    "plt.title('Depth Visualization')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
